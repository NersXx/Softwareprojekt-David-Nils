{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcc99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_9800\\2218620142.py:19: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  plt.scatter(xy_train[:, :, 0], xy_train[:, :, 1], cmap='hsv', s=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100, 2)\n",
      "(10000, 1)\n",
      "(10000, 100, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_norm shape: [[[-1.3425372e-05 -1.5320942e-04]\n",
      "  [-5.2129768e-02 -1.0401086e-02]\n",
      "  [-6.9177844e-02 -2.8402818e-02]\n",
      "  ...\n",
      "  [-5.5069170e+00  2.2490942e+00]\n",
      "  [-3.5734904e+00  7.0251608e-01]\n",
      "  [-5.8230047e+00 -1.5320942e-04]]\n",
      "\n",
      " [[-1.3425372e-05 -1.5320942e-04]\n",
      "  [ 1.2320579e-02  2.2720827e-03]\n",
      "  [ 3.2359481e-02  1.3069224e-02]\n",
      "  ...\n",
      "  [ 1.0404962e+00 -4.2514038e-01]\n",
      "  [ 1.4234835e+00 -2.8006196e-01]\n",
      "  [ 1.6391985e+00 -1.5320942e-04]]\n",
      "\n",
      " [[-1.3425372e-05 -1.5320942e-04]\n",
      "  [-2.5499802e-02 -5.1647131e-03]\n",
      "  [-3.8330507e-02 -1.5803492e-02]\n",
      "  ...\n",
      "  [-2.1143448e+00  8.6342722e-01]\n",
      "  [-2.0266342e+00  3.9835057e-01]\n",
      "  [-2.9603000e+00 -1.5320942e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.3425372e-05 -1.5320942e-04]\n",
      "  [ 1.7113455e-02  3.2145279e-03]\n",
      "  [ 2.6770536e-02  1.0786469e-02]\n",
      "  ...\n",
      "  [ 1.2105616e+00 -4.9460211e-01]\n",
      "  [ 1.3925500e+00 -2.7397934e-01]\n",
      "  [ 1.2772022e+00 -1.5320942e-04]]\n",
      "\n",
      " [[-1.3425372e-05 -1.5320942e-04]\n",
      "  [-9.6650720e-03 -2.0510573e-03]\n",
      "  [-1.8435311e-02 -7.6774699e-03]\n",
      "  ...\n",
      "  [-7.7804470e-01  3.1762698e-01]\n",
      "  [-8.4502107e-01  1.6600454e-01]\n",
      "  [-8.3106416e-01 -1.5320942e-04]]\n",
      "\n",
      " [[-1.3425372e-05 -1.5320942e-04]\n",
      "  [ 1.7708424e-02  3.3315194e-03]\n",
      "  [ 3.2794874e-02  1.3247058e-02]\n",
      "  ...\n",
      "  [ 2.7609935e+00 -1.1278627e+00]\n",
      "  [ 2.5353196e+00 -4.9868742e-01]\n",
      "  [ 2.0941086e+00 -1.5320942e-04]]]\n",
      "alpha_norm shape: [[-0.2892316 ]\n",
      " [ 0.16464639]\n",
      " [ 2.059859  ]\n",
      " ...\n",
      " [-0.335412  ]\n",
      " [-0.11873462]\n",
      " [-0.25457346]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import dataset 'spirals.npz' from local directory\n",
    "data = np.load('spirals.npz')\n",
    "xy_train = data['xy_train']\n",
    "alpha_train = data['alpha_train']\n",
    "xy_test = data['xy_test']\n",
    "#plot xy_train (Training data of shape (10000, 100, 2). This represents 10,000 spiral trajectories, each with 100 time points and 2 spatial dimensions (x, y).)\n",
    "plt.scatter(xy_train[:, :, 0], xy_train[:, :, 1], cmap='hsv', s=1)\n",
    "plt.title('Training Data: Spiral Trajectories')\n",
    "print(xy_train.shape)\n",
    "print(alpha_train.shape)\n",
    "print(xy_test.shape)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy = data[\"xy_train\"].astype(np.float32)   # (N, T, D)\n",
    "alpha = data[\"alpha_train\"].astype(np.float32)\n",
    "\n",
    "# compute train stats\n",
    "mean = xy.mean(axis=(0,1), keepdims=True)   # shape (1,1,D)\n",
    "std  = xy.std(axis=(0,1), keepdims=True)\n",
    "xy_norm = (xy - mean) / (std + 1e-6)\n",
    "\n",
    "alpha_mean = alpha.mean()\n",
    "alpha_std  = alpha.std()\n",
    "alpha_norm = (alpha - alpha_mean) / (alpha_std + 1e-6)\n",
    "\n",
    "# convert to jax\n",
    "xy_norm = jnp.array(xy_norm)\n",
    "alpha_norm = jnp.array(alpha_norm)\n",
    "\n",
    "T = xy_norm.shape[1]\n",
    "t = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)   # shape (T,)\n",
    "# Now, xy_norm contains the normalized spiral trajectories, and alpha_norm contains the normalized parameters.\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def batch_generator(xy, alpha, batch_size, shuffle=True):\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch_idx = idx[i:i+batch_size]\n",
    "        yield xy[batch_idx], alpha[batch_idx]\n",
    "\n",
    "global def target(params, h, alpha):\n",
    "\n",
    "\n",
    "\n",
    "def train_step(params, opt_state, x_batch, t, key):\n",
    "    def loss_fn(params):\n",
    "        h = encoder_init(params, x_batch[:, 0, :])   # (B, h_dim)\n",
    "        total_loss = 0.0\n",
    "        for i in range(T-1):\n",
    "            # integrate from t[i] to t[i+1]\n",
    "            sol = diffeqsolve(ODETerm(ode_func(params)), solver, t0=t[i], t1=t[i+1], y0=h)\n",
    "            h = sol.ys[-1]\n",
    "            # predict next-step\n",
    "            y_pred = decoder(params, h)   # (B, D)\n",
    "            total_loss += ((y_pred - x_batch[:, i+1, :])**2).mean()\n",
    "            # RNN update with observed x at i+1\n",
    "            h = rnn_cell(params, x_batch[:, i+1, :], h)\n",
    "        return total_loss / (T-1)\n",
    "\n",
    "    grads = jax.grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071eb62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f6393",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (2,) and (1,).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    239\u001b[39m xb_j = jnp.array(xb)\n\u001b[32m    240\u001b[39m ab_j = jnp.array(ab)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m loss_val, grads = \u001b[43mloss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m model, opt_state = apply_updates(model, grads, opt_state)\n\u001b[32m    243\u001b[39m losses.append(\u001b[38;5;28mfloat\u001b[39m(loss_val))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mloss_and_grad\u001b[39m\u001b[34m(model, x_batch, alpha_batch)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m l + l2\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Berechne den Gradienten bezüglich der Parameter 'params'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m loss_val, grads = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_loss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss_val, grads\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_ad.py:71\u001b[39m, in \u001b[36m_ValueAndGradWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m x, *args = args\n\u001b[32m     70\u001b[39m diff_x, nondiff_x = partition(x, is_inexact_array)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnondiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 11 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mloss_and_grad.<locals>._loss_fn\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_loss_fn\u001b[39m(p):\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# Übergebe die Parameter an sequence_loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     l, _ = \u001b[43msequence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# Weight decay\u001b[39;00m\n\u001b[32m    192\u001b[39m     leaves = jax.tree_util.tree_leaves(p)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36msequence_loss\u001b[39m\u001b[34m(params, model, x_seq, alpha_true)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Ersetze die statischen Teile des Modells (die Odefunc) \u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# mit den trainierbaren Parametern\u001b[39;00m\n\u001b[32m    164\u001b[39m model = eqx.combine(model, params)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m h = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, h_dim)\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# vmap-bare Funktion zum Lösen der ODE\u001b[39;00m\n\u001b[32m    169\u001b[39m vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mEncoder.__call__\u001b[39m\u001b[34m(self, x0)\u001b[39m\n\u001b[32m     51\u001b[39m x = jnp.atleast_2d(x0)      \u001b[38;5;66;03m# (B, in_size) oder (1, in_size)\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# direct call: eqx.nn.MLP unterstützt batched inputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# -> (B, out_size)\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# optional assert (nur zum Debuggen)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# assert out.shape[0] == x.shape[0] and out.shape[1] == self.net.out_size\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_mlp.py:140\u001b[39m, in \u001b[36mMLP.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"**Arguments:**\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03m- `x`: A JAX array with shape `(in_size,)`. (Or shape `()` if\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03mA JAX array with shape `(out_size,)`. (Or shape `()` if `out_size=\"scalar\"`.)\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers[:-\u001b[32m1\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     layer_activation = jtu.tree_map(\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[i] \u001b[38;5;28;01mif\u001b[39;00m is_array(x) \u001b[38;5;28;01melse\u001b[39;00m x, \u001b[38;5;28mself\u001b[39m.activation\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    144\u001b[39m     x = filter_vmap(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: a(b))(layer_activation, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_linear.py:97\u001b[39m, in \u001b[36mLinear.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mx must have scalar shape\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m     x = jnp.broadcast_to(x, (\u001b[32m1\u001b[39m,))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     99\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.bias\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:604\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    602\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\tensor_contractions.py:254\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m    252\u001b[39m a = lax.squeeze(a, \u001b[38;5;28mtuple\u001b[39m(a_squeeze))\n\u001b[32m    253\u001b[39m b = lax.squeeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m out = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m result = lax.transpose(out, perm)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lax._convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:5303\u001b[39m, in \u001b[36m_dot_general_shape_rule\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   5300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core.definitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[32m   5301\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5302\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5303\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[32m   5305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
      "\u001b[31mTypeError\u001b[39m: dot_general requires contracting dimensions to have the same shape, got (2,) and (1,)."
     ]
    }
   ],
   "source": [
    "# ode_rnn_alpha_fixed.py\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train = data[\"xy_train\"].astype(np.float32)   # (N, T, 2)\n",
    "alpha_train = data[\"alpha_train\"].astype(np.float32)  # (N, 1)\n",
    "xy_test = data[\"xy_test\"].astype(np.float32)\n",
    "\n",
    "N, T, D = xy_train.shape\n",
    "assert D == 2\n",
    "\n",
    "# normalize inputs (per-dimension)\n",
    "xy_mean = xy_train.mean(axis=(0,1), keepdims=True)\n",
    "xy_std  = xy_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "xy_train_n = (xy_train - xy_mean) / xy_std\n",
    "xy_test_n  = (xy_test - xy_mean) / xy_std\n",
    "\n",
    "alpha_mean = alpha_train.mean(axis=0, keepdims=True)\n",
    "alpha_std  = alpha_train.std(axis=0, keepdims=True) + 1e-6\n",
    "alpha_train_n = (alpha_train - alpha_mean) / alpha_std\n",
    "\n",
    "# convert to jax\n",
    "xy_train_n = jnp.array(xy_train_n)\n",
    "alpha_train_n = jnp.array(alpha_train_n)\n",
    "xy_test_n = jnp.array(xy_test_n)\n",
    "\n",
    "# temporal grid: assume observations evenly spaced in [0,1]\n",
    "t_grid = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)\n",
    "\n",
    "# Model components (Equinox)\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        x = jnp.atleast_2d(x0)      # (B, in_size) oder (1, in_size)\n",
    "        # direct call: eqx.nn.MLP unterstützt batched inputs\n",
    "        out = self.net(x)          # -> (B, out_size)\n",
    "        # optional assert (nur zum Debuggen)\n",
    "        # assert out.shape[0] == x.shape[0] and out.shape[1] == self.net.out_size\n",
    "        return out\n",
    "    \n",
    "def ensure_batched(x):\n",
    "    x = jnp.asarray(x)\n",
    "    if x.ndim == 0:\n",
    "        return x.reshape((1, 1))\n",
    "    if x.ndim == 1:\n",
    "        return x[None, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "class ODEFunc(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, h_dim, *, key):\n",
    "        # Das MLP erwartet jetzt h_dim (Zustand) + 1 (Zeit)\n",
    "        self.net = eqx.nn.MLP(h_dim + 1, h_dim, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        # h ist jetzt (h_dim,)\n",
    "        # t ist ein Skalar\n",
    "        \n",
    "        # t in einen Vektor der Form (1,) umwandeln\n",
    "        t_feat = jnp.atleast_1d(t) \n",
    "        \n",
    "        # h und t_feat verketten\n",
    "        inp = jnp.concatenate([h, t_feat]) # -> (h_dim + 1,)\n",
    "        out = self.net(inp)                # -> (h_dim,)\n",
    "        return out\n",
    "\n",
    "class Decoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=64, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        h_mat = jnp.atleast_2d(h)   # (B, h_dim)\n",
    "        out = self.net(h_mat)       # -> (B, out_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNNUpdate(eqx.Module):\n",
    "    lin_x: eqx.nn.Linear\n",
    "    lin_h: eqx.nn.Linear\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, key):\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        self.lin_x = eqx.nn.Linear(x_dim, 3 * h_dim, key=k1)\n",
    "        self.lin_h = eqx.nn.Linear(h_dim, 3 * h_dim, key=k2)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        x = jnp.atleast_2d(x)   # (B, x_dim)\n",
    "        h = jnp.atleast_2d(h)   # (B, h_dim)\n",
    "        gates = self.lin_x(x) + self.lin_h(h)  # (B, 3*h_dim)\n",
    "        z, r, o = jnp.split(gates, 3, axis=-1)\n",
    "        z = jnn.sigmoid(z); r = jnn.sigmoid(r); o = jnp.tanh(o)\n",
    "        h_new = (1 - z) * h + z * o\n",
    "        return h_new\n",
    "\n",
    "class ODERNN(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    odefunc: ODEFunc\n",
    "    rnn_update: RNNUpdate\n",
    "    decoder: Decoder\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, *, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.encoder = Encoder(x_dim, h_dim, key=k1)\n",
    "        self.odefunc = ODEFunc(h_dim, key=k2)\n",
    "        self.rnn_update = RNNUpdate(x_dim, h_dim, key=k3)\n",
    "        self.decoder = Decoder(h_dim, 1, key=k4)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "# Solver and utils\n",
    "# Put tolerances on solver construction to avoid passing rtol/atol into diffeqsolve\n",
    "# Solver and utils (robuste Version mit dt0)\n",
    "\n",
    "solver = diffrax.Tsit5()\n",
    "adjoint = diffrax.BacksolveAdjoint()\n",
    "# Den Controller hier definieren, um ihn wiederzuverwenden\n",
    "stepsize_controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
    "# Der ODETerm wird jetzt außerhalb der Verlustfunktion definiert\n",
    "# und nimmt das *statische* ODERNN-Modell entgegen\n",
    "term = diffrax.ODETerm(ODEFunc(h_dim=h_dim, key=jax.random.PRNGKey(0))) # Dummy-Key, wird überschrieben\n",
    "\n",
    "def mse(a, b):\n",
    "    return jnp.mean((a - b) ** 2)\n",
    "\n",
    "def solve_ode_vmap(odefunc, h0, t0, t1):\n",
    "    \"\"\"Löst die ODE für einen einzelnen Batch-Eintrag.\"\"\"\n",
    "    term = diffrax.ODETerm(odefunc)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        y0=h0,\n",
    "        args=None, # odefunc ist jetzt der Term selbst\n",
    "        dt0=None,  # Automatisch\n",
    "        stepsize_controller=stepsize_controller,\n",
    "        adjoint=adjoint,\n",
    "    )\n",
    "    # sol.ys hat die Form (1, h_dim)\n",
    "    return sol.ys[-1]\n",
    "\n",
    "def sequence_loss(params, model: ODERNN, x_seq, alpha_true):\n",
    "    # x_seq: (B, T, D)\n",
    "    B = x_seq.shape[0]\n",
    "    \n",
    "    # Ersetze die statischen Teile des Modells (die Odefunc) \n",
    "    # mit den trainierbaren Parametern\n",
    "    model = eqx.combine(model, params)\n",
    "    \n",
    "    h = jax.vmap(model.encoder)(x_seq[:, 0, :])  # (B, h_dim)\n",
    "    \n",
    "    # vmap-bare Funktion zum Lösen der ODE\n",
    "    vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(None, 0, None, None))\n",
    "    \n",
    "    for i in range(T - 1):\n",
    "        # Führe die ODE-Integration für den gesamten Batch mit vmap aus\n",
    "        h = vmapped_solve(model.odefunc, h, float(t_grid[i]), float(t_grid[i+1]))\n",
    "        \n",
    "        # Führe den RNN-Update für den gesamten Batch aus (RNNUpdate unterstützt Batching)\n",
    "        h = model.rnn_update(x_seq[:, i+1, :], h)\n",
    "        \n",
    "    # Decoder für den gesamten Batch (Decoder unterstützt Batching)\n",
    "    alpha_pred = model.decoder(h)  # (B,1)\n",
    "    loss = mse(alpha_pred, alpha_true)\n",
    "    return loss, alpha_pred\n",
    "\n",
    "def loss_and_grad(model, x_batch, alpha_batch):\n",
    "    # Filtere nur die trainierbaren Parameter (Arrays) heraus\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    \n",
    "    def _loss_fn(p):\n",
    "        # Übergebe die Parameter an sequence_loss\n",
    "        l, _ = sequence_loss(p, model, x_batch, alpha_batch)\n",
    "        \n",
    "        # Weight decay\n",
    "        leaves = jax.tree_util.tree_leaves(p)\n",
    "        l2 = 1e-6 * sum(jnp.sum(p ** 2) for p in leaves)\n",
    "        return l + l2\n",
    "        \n",
    "    # Berechne den Gradienten bezüglich der Parameter 'params'\n",
    "    loss_val, grads = eqx.filter_value_and_grad(_loss_fn)(params)\n",
    "    return loss_val, grads\n",
    "\n",
    "# Training loop\n",
    "key = random.PRNGKey(0)\n",
    "h_dim = 64\n",
    "model = ODERNN(x_dim=D, h_dim=h_dim, key=key)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 5e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "# Opt-State wird auf die trainierbaren Parameter initialisiert\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "def data_loader(xy, alpha, batch_size, shuffle=True):\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        b = idx[i:i+batch_size]\n",
    "        yield xy[b], alpha[b]\n",
    "\n",
    "# -----------------------------\n",
    "# ANGEPASSTE FUNKTION: apply_updates\n",
    "# -----------------------------\n",
    "@jax.jit\n",
    "def apply_updates(model, grads, opt_state):\n",
    "    # Filtere die Parameter, die aktualisiert werden sollen\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    # Wende Updates auf das Modell an\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    for xb, ab in data_loader(np.array(xy_train_n), np.array(alpha_train_n), batch_size):\n",
    "        xb_j = jnp.array(xb)\n",
    "        ab_j = jnp.array(ab)\n",
    "        loss_val, grads = loss_and_grad(model, xb_j, ab_j)\n",
    "        model, opt_state = apply_updates(model, grads, opt_state)\n",
    "        losses.append(float(loss_val))\n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} loss={np.mean(losses):.6f} time={t1-t0:.1f}s\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ANGEPASSTE FUNKTION: predict_alpha\n",
    "# -----------------------------\n",
    "def predict_alpha(model, xy_input):\n",
    "    # xy_input: (N, T, D) normalized\n",
    "    batch_size = 128\n",
    "    preds = []\n",
    "    \n",
    "    # Definiere die vmap-bare Solve-Funktion (genau wie im Training)\n",
    "    vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(None, 0, None, None))\n",
    "\n",
    "    for i in range(0, xy_input.shape[0], batch_size):\n",
    "        xb = xy_input[i:i+batch_size]\n",
    "        B = xb.shape[0]\n",
    "        \n",
    "        # Encoder (unterstützt Batching)\n",
    "        h = model.encoder(xb[:, 0, :])\n",
    "        \n",
    "        for j in range(T - 1):\n",
    "            # ODE-Integration per vmap\n",
    "            h = vmapped_solve(model.odefunc, h, float(t_grid[j]), float(t_grid[j+1]))\n",
    "            # RNN-Update (unterstützt Batching)\n",
    "            h = model.rnn_update(xb[:, j+1, :], h)\n",
    "            \n",
    "        # Decoder (unterstützt Batching)\n",
    "        alpha_p = model.decoder(h)  # (B,1) normalized\n",
    "        preds.append(np.array(alpha_p))\n",
    "        \n",
    "    preds = np.vstack(preds)\n",
    "    # denormalize\n",
    "    preds_orig = preds * alpha_std + alpha_mean\n",
    "    return preds_orig\n",
    "\n",
    "# ... [Der restliche Code (Saving, Plotting) bleibt gleich] ...\n",
    "\n",
    "# Schnelle Anpassung für die Plot-Schleife am Ende\n",
    "idx = 0\n",
    "xb = xy_train_n[idx:idx+1]  # (1, T, D)\n",
    "h = model.encoder(xb[:, 0, :]) # (1, h_dim)\n",
    "alphas_steps = []\n",
    "\n",
    "# vmap-bare Funktion (auch für Batch-Größe 1)\n",
    "vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(None, 0, None, None))\n",
    "\n",
    "for j in range(T - 1):\n",
    "    h = vmapped_solve(model.odefunc, h, float(t_grid[j]), float(t_grid[j+1]))\n",
    "    h = model.rnn_update(xb[:, j+1, :], h)\n",
    "    a = np.array(model.decoder(h))[0,0] * alpha_std + alpha_mean\n",
    "    alphas_steps.append(float(a))\n",
    "\n",
    "true_alpha_val = float(alpha_train[idx, 0])\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(alphas_steps)), alphas_steps, label=\"predicted alpha (per step)\")\n",
    "plt.hlines(true_alpha_val, 0, len(alphas_steps)-1, colors='r', linestyles='--', label=\"true alpha\")\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"alpha\")\n",
    "plt.legend()\n",
    "plt.title(f\"Alpha over time for example {idx}\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11d1c243",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (2,) and (128,).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 232\u001b[39m\n\u001b[32m    230\u001b[39m xb_j = jnp.array(xb)\n\u001b[32m    231\u001b[39m ab_j = jnp.array(ab)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m loss_val, grads = \u001b[43mloss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m model, opt_state = apply_updates(model, grads, opt_state)\n\u001b[32m    234\u001b[39m losses.append(\u001b[38;5;28mfloat\u001b[39m(loss_val))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 191\u001b[39m, in \u001b[36mloss_and_grad\u001b[39m\u001b[34m(model, x_batch, alpha_batch)\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m l + l2\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Berechne den Gradienten bezüglich der Parameter 'params'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m loss_val, grads = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_loss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss_val, grads\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_ad.py:71\u001b[39m, in \u001b[36m_ValueAndGradWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m x, *args = args\n\u001b[32m     70\u001b[39m diff_x, nondiff_x = partition(x, is_inexact_array)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnondiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 11 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mloss_and_grad.<locals>._loss_fn\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_loss_fn\u001b[39m(p):\n\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# Übergebe die Parameter an sequence_loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     l, _ = \u001b[43msequence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# Weight decay\u001b[39;00m\n\u001b[32m    186\u001b[39m     leaves = jax.tree_util.tree_leaves(p)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36msequence_loss\u001b[39m\u001b[34m(params, model, x_seq, alpha_true)\u001b[39m\n\u001b[32m    155\u001b[39m model = eqx.combine(model, params)\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------------------\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# KORREKTUR HIER: jax.vmap entfernt, da Encoder Batching unterstützt\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m h = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, h_dim)\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# vmap-bare Funktion zum Lösen der ODE\u001b[39;00m\n\u001b[32m    163\u001b[39m vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mEncoder.__call__\u001b[39m\u001b[34m(self, x0)\u001b[39m\n\u001b[32m     51\u001b[39m x = jnp.atleast_2d(x0)      \u001b[38;5;66;03m# (B, in_size) oder (1, in_size)\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# direct call: eqx.nn.MLP unterstützt batched inputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# -> (B, out_size)\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_mlp.py:140\u001b[39m, in \u001b[36mMLP.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"**Arguments:**\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03m- `x`: A JAX array with shape `(in_size,)`. (Or shape `()` if\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03mA JAX array with shape `(out_size,)`. (Or shape `()` if `out_size=\"scalar\"`.)\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers[:-\u001b[32m1\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     layer_activation = jtu.tree_map(\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[i] \u001b[38;5;28;01mif\u001b[39;00m is_array(x) \u001b[38;5;28;01melse\u001b[39;00m x, \u001b[38;5;28mself\u001b[39m.activation\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    144\u001b[39m     x = filter_vmap(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: a(b))(layer_activation, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_linear.py:97\u001b[39m, in \u001b[36mLinear.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mx must have scalar shape\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m     x = jnp.broadcast_to(x, (\u001b[32m1\u001b[39m,))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     99\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.bias\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:604\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    602\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\tensor_contractions.py:254\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m    252\u001b[39m a = lax.squeeze(a, \u001b[38;5;28mtuple\u001b[39m(a_squeeze))\n\u001b[32m    253\u001b[39m b = lax.squeeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m out = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m result = lax.transpose(out, perm)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lax._convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:5303\u001b[39m, in \u001b[36m_dot_general_shape_rule\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   5300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core.definitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[32m   5301\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5302\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5303\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[32m   5305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
      "\u001b[31mTypeError\u001b[39m: dot_general requires contracting dimensions to have the same shape, got (2,) and (128,)."
     ]
    }
   ],
   "source": [
    "# ode_rnn_alpha_fixed.py\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train = data[\"xy_train\"].astype(np.float32)   # (N, T, 2)\n",
    "alpha_train = data[\"alpha_train\"].astype(np.float32)  # (N, 1)\n",
    "xy_test = data[\"xy_test\"].astype(np.float32)\n",
    "\n",
    "N, T, D = xy_train.shape\n",
    "assert D == 2\n",
    "\n",
    "# normalize inputs (per-dimension)\n",
    "xy_mean = xy_train.mean(axis=(0,1), keepdims=True)\n",
    "xy_std  = xy_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "xy_train_n = (xy_train - xy_mean) / xy_std\n",
    "xy_test_n  = (xy_test - xy_mean) / xy_std\n",
    "\n",
    "alpha_mean = alpha_train.mean(axis=0, keepdims=True)\n",
    "alpha_std  = alpha_train.std(axis=0, keepdims=True) + 1e-6\n",
    "alpha_train_n = (alpha_train - alpha_mean) / alpha_std\n",
    "\n",
    "# convert to jax\n",
    "xy_train_n = jnp.array(xy_train_n)\n",
    "alpha_train_n = jnp.array(alpha_train_n)\n",
    "xy_test_n = jnp.array(xy_test_n)\n",
    "\n",
    "# temporal grid: assume observations evenly spaced in [0,1]\n",
    "t_grid = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)\n",
    "\n",
    "# Model components (Equinox)\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        x = jnp.atleast_2d(x0)      # (B, in_size) oder (1, in_size)\n",
    "        # direct call: eqx.nn.MLP unterstützt batched inputs\n",
    "        out = self.net(x)          # -> (B, out_size)\n",
    "        return out\n",
    "\n",
    "class ODEFunc(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, h_dim, *, key):\n",
    "        # Das MLP erwartet jetzt h_dim (Zustand) + 1 (Zeit)\n",
    "        self.net = eqx.nn.MLP(h_dim + 1, h_dim, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        # h ist jetzt (h_dim,)\n",
    "        # t ist ein Skalar\n",
    "        \n",
    "        # t in einen Vektor der Form (1,) umwandeln\n",
    "        t_feat = jnp.atleast_1d(t) \n",
    "        \n",
    "        # h und t_feat verketten\n",
    "        inp = jnp.concatenate([h, t_feat]) # -> (h_dim + 1,)\n",
    "        out = self.net(inp)                # -> (h_dim,)\n",
    "        return out\n",
    "\n",
    "class Decoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=64, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        h_mat = jnp.atleast_2d(h)  # (B, h_dim)\n",
    "        out = self.net(h_mat)      # -> (B, out_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNNUpdate(eqx.Module):\n",
    "    lin_x: eqx.nn.Linear\n",
    "    lin_h: eqx.nn.Linear\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, key):\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        self.lin_x = eqx.nn.Linear(x_dim, 3 * h_dim, key=k1)\n",
    "        self.lin_h = eqx.nn.Linear(h_dim, 3 * h_dim, key=k2)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        x = jnp.atleast_2d(x)  # (B, x_dim)\n",
    "        h = jnp.atleast_2d(h)  # (B, h_dim)\n",
    "        gates = self.lin_x(x) + self.lin_h(h)  # (B, 3*h_dim)\n",
    "        z, r, o = jnp.split(gates, 3, axis=-1)\n",
    "        z = jnn.sigmoid(z); r = jnn.sigmoid(r); o = jnp.tanh(o)\n",
    "        h_new = (1 - z) * h + z * o\n",
    "        return h_new\n",
    "\n",
    "class ODERNN(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    odefunc: ODEFunc\n",
    "    rnn_update: RNNUpdate\n",
    "    decoder: Decoder\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, *, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.encoder = Encoder(x_dim, h_dim, key=k1)\n",
    "        self.odefunc = ODEFunc(h_dim, key=k2)\n",
    "        self.rnn_update = RNNUpdate(x_dim, h_dim, key=k3)\n",
    "        self.decoder = Decoder(h_dim, 1, key=k4)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "# Solver and utils\n",
    "solver = diffrax.Tsit5()\n",
    "adjoint = diffrax.BacksolveAdjoint()\n",
    "stepsize_controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
    "\n",
    "def mse(a, b):\n",
    "    return jnp.mean((a - b) ** 2)\n",
    "\n",
    "def solve_ode_vmap(odefunc, h0, t0, t1):\n",
    "    \"\"\"Löst die ODE für einen einzelnen Batch-Eintrag.\"\"\"\n",
    "    term = diffrax.ODETerm(odefunc)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        y0=h0,\n",
    "        args=None, # odefunc ist jetzt der Term selbst\n",
    "        dt0=None,  # Automatisch\n",
    "        stepsize_controller=stepsize_controller,\n",
    "        adjoint=adjoint,\n",
    "    )\n",
    "    # sol.ys hat die Form (1, h_dim)\n",
    "    return sol.ys[-1]\n",
    "\n",
    "def sequence_loss(params, model: ODERNN, x_seq, alpha_true):\n",
    "    # x_seq: (B, T, D)\n",
    "    B = x_seq.shape[0]\n",
    "    \n",
    "    # Ersetze die statischen Teile des Modells (die Odefunc) \n",
    "    # mit den trainierbaren Parametern\n",
    "    model = eqx.combine(model, params)\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # KORREKTUR HIER: jax.vmap entfernt, da Encoder Batching unterstützt\n",
    "    # -----------------------------------------------------------------\n",
    "    h = model.encoder(x_seq[:, 0, :])  # (B, h_dim)\n",
    "    \n",
    "    # vmap-bare Funktion zum Lösen der ODE\n",
    "    vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(None, 0, None, None))\n",
    "    \n",
    "    for i in range(T - 1):\n",
    "        # Führe die ODE-Integration für den gesamten Batch mit vmap aus\n",
    "        h = vmapped_solve(model.odefunc, h, float(t_grid[i]), float(t_grid[i+1]))\n",
    "        \n",
    "        # Führe den RNN-Update für den gesamten Batch aus (RNNUpdate unterstützt Batching)\n",
    "        h = model.rnn_update(x_seq[:, i+1, :], h)\n",
    "        \n",
    "    # Decoder für den gesamten Batch (Decoder unterstützt Batching)\n",
    "    alpha_pred = model.decoder(h)  # (B,1)\n",
    "    loss = mse(alpha_pred, alpha_true)\n",
    "    return loss, alpha_pred\n",
    "\n",
    "def loss_and_grad(model, x_batch, alpha_batch):\n",
    "    # Filtere nur die trainierbaren Parameter (Arrays) heraus\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    \n",
    "    def _loss_fn(p):\n",
    "        # Übergebe die Parameter an sequence_loss\n",
    "        l, _ = sequence_loss(p, model, x_batch, alpha_batch)\n",
    "        \n",
    "        # Weight decay\n",
    "        leaves = jax.tree_util.tree_leaves(p)\n",
    "        l2 = 1e-6 * sum(jnp.sum(p ** 2) for p in leaves)\n",
    "        return l + l2\n",
    "        \n",
    "    # Berechne den Gradienten bezüglich der Parameter 'params'\n",
    "    loss_val, grads = eqx.filter_value_and_grad(_loss_fn)(params)\n",
    "    return loss_val, grads\n",
    "\n",
    "# Training loop\n",
    "key = random.PRNGKey(0)\n",
    "h_dim = 64\n",
    "model = ODERNN(x_dim=D, h_dim=h_dim, key=key)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 5e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "# Opt-State wird auf die trainierbaren Parameter initialisiert\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "def data_loader(xy, alpha, batch_size, shuffle=True):\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        b = idx[i:i+batch_size]\n",
    "        yield xy[b], alpha[b]\n",
    "\n",
    "@jax.jit\n",
    "def apply_updates(model, grads, opt_state):\n",
    "    # Filtere die Parameter, die aktualisiert werden sollen\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    # Wende Updates auf das Modell an\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    for xb, ab in data_loader(np.array(xy_train_n), np.array(alpha_train_n), batch_size):\n",
    "        xb_j = jnp.array(xb)\n",
    "        ab_j = jnp.array(ab)\n",
    "        loss_val, grads = loss_and_grad(model, xb_j, ab_j)\n",
    "        model, opt_state = apply_updates(model, grads, opt_state)\n",
    "        losses.append(float(loss_val))\n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} loss={np.mean(losses):.6f} time={t1-t0:.1f}s\")\n",
    "\n",
    "\n",
    "def predict_alpha(model, xy_input):\n",
    "    # xy_input: (N, T, D) normalized\n",
    "    batch_size = 128\n",
    "    preds = []\n",
    "    \n",
    "    # Definiere die vmap-bare Solve-Funktion (genau wie im Training)\n",
    "    vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(None, 0, None, None))\n",
    "\n",
    "    for i in range(0, xy_input.shape[0], batch_size):\n",
    "        xb = xy_input[i:i+batch_size]\n",
    "        B = xb.shape[0]\n",
    "        \n",
    "        # Encoder (unterstützt Batching)\n",
    "        h = model.encoder(xb[:, 0, :])\n",
    "        \n",
    "        for j in range(T - 1):\n",
    "            # ODE-Integration per vmap\n",
    "            h = vmapped_solve(model.odefunc, h, float(t_grid[j]), float(t_grid[j+1]))\n",
    "            # RNN-Update (unterstützt Batching)\n",
    "            h = model.rnn_update(xb[:, j+1, :], h)\n",
    "            \n",
    "        # Decoder (unterstützt Batching)\n",
    "        alpha_p = model.decoder(h)  # (B,1) normalized\n",
    "        preds.append(np.array(alpha_p))\n",
    "        \n",
    "    preds = np.vstack(preds)\n",
    "    # denormalize\n",
    "    preds_orig = preds * alpha_std + alpha_mean\n",
    "    return preds_orig\n",
    "\n",
    "alpha_test_pred = predict_alpha(model, xy_test_n)  # shape (N,1)\n",
    "np.save(\"alpha_test_pred.npy\", alpha_test_pred)\n",
    "print(\"Saved alpha_test_pred.npy with shape\", alpha_test_pred.shape)\n",
    "\n",
    "# optional quick eval on train set (to sanity-check) and plot\n",
    "subset = 200\n",
    "train_pred = predict_alpha(model, xy_train_n[:subset])\n",
    "train_true = np.array(alpha_train[:subset])\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(train_true.ravel(), train_pred.ravel(), alpha=0.6, s=20, label=\"pred vs true\")\n",
    "minv = min(train_true.min(), train_pred.min())\n",
    "maxv = max(train_true.max(), train_pred.max())\n",
    "plt.plot([minv, maxv], [minv, maxv], 'r--', label=\"identity\")\n",
    "plt.xlabel(\"alpha true\")\n",
    "plt.ylabel(\"alpha predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Predicted alpha vs True alpha (subset)\")\n",
    "plt.show()\n",
    "\n",
    "# Example: alpha over time for a single example (predicted after each step) and compare to true alpha\n",
    "idx = 0\n",
    "xb = xy_train_n[idx:idx+1]  # (1, T, D)\n",
    "h = model.encoder(xb[:, 0, :]) # (1, h_dim)\n",
    "alphas_steps = []\n",
    "\n",
    "# vmap-bare Funktion (auch für Batch-Größe 1)\n",
    "vmapped_solve = jax.vmap(solve_ode_vmap, in_axes=(None, 0, None, None))\n",
    "\n",
    "for j in range(T - 1):\n",
    "    h = vmapped_solve(model.odefunc, h, float(t_grid[j]), float(t_grid[j+1]))\n",
    "    h = model.rnn_update(xb[:, j+1, :], h)\n",
    "    a = np.array(model.decoder(h))[0,0] * alpha_std + alpha_mean\n",
    "    alphas_steps.append(float(a))\n",
    "\n",
    "true_alpha_val = float(alpha_train[idx, 0])\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(alphas_steps)), alphas_steps, label=\"predicted alpha (per step)\")\n",
    "plt.hlines(true_alpha_val, 0, len(alphas_steps)-1, colors='r', linestyles='--', label=\"true alpha\")\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"alpha\")\n",
    "plt.legend()\n",
    "plt.title(f\"Alpha over time for example {idx}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2e1a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Terms are not compatible with solver! Got:\nODETerm(vector_field=<function integrate_batch.<locals>.rhs_flat>)\nbut expected:\ndiffrax.AbstractTerm\nNote that terms are checked recursively: if you scroll up you may find a root-cause error that is more specific.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:170\u001b[39m, in \u001b[36m_assert_term_compatible.<locals>._check\u001b[39m\u001b[34m(term_cls, term, term_contr_kwargs, yi)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     vf_type = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_eval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_eval_shape.py:38\u001b[39m, in \u001b[36mfilter_eval_shape\u001b[39m\u001b[34m(fun, *args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m dynamic, static = partition((fun, args, kwargs), _filter)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m dynamic_out, static_out = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combine(dynamic_out, static_out.value)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api.py:3062\u001b[39m, in \u001b[36meval_shape\u001b[39m\u001b[34m(fun, *args, **kwargs)\u001b[39m\n\u001b[32m   3061\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m: fun = partial(fun)\n\u001b[32m-> \u001b[39m\u001b[32m3062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.out_info\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:305\u001b[39m, in \u001b[36mjit_trace\u001b[39m\u001b[34m(jit_func, *args, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mjit_trace\u001b[39m(jit_func, *args, **kwargs) -> stages.Traced:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjit_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m   lower_callable = partial(_resolve_and_lower, args_flat, pgle_profiler=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_eval_shape.py:33\u001b[39m, in \u001b[36mfilter_eval_shape.<locals>._fn\u001b[39m\u001b[34m(_static, _dynamic)\u001b[39m\n\u001b[32m     32\u001b[39m _fun, _args, _kwargs = combine(_static, _dynamic)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m _out = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m _dynamic_out, _static_out = partition(_out, _filter)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_term.py:194\u001b[39m, in \u001b[36mODETerm.vf\u001b[39m\u001b[34m(self, t, y, args)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mvf\u001b[39m(\u001b[38;5;28mself\u001b[39m, t: RealScalarLike, y: Y, args: Args) -> _VF:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m jtu.tree_structure(out) != jtu.tree_structure(y):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mintegrate_batch.<locals>.rhs_flat\u001b[39m\u001b[34m(t, y, args)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# 2. ODEFunc aufrufen: (B, h_dim) -> (B, h_dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m dydt = \u001b[43modefunc_from_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# 3. Rückgabe muss (B * h_dim,) sein\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mODEFunc.__call__\u001b[39m\u001b[34m(self, t, h, args)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# MLP durchlaufen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, h_dim)\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_mlp.py:140\u001b[39m, in \u001b[36mMLP.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers[:-\u001b[32m1\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     layer_activation = jtu.tree_map(\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[i] \u001b[38;5;28;01mif\u001b[39;00m is_array(x) \u001b[38;5;28;01melse\u001b[39;00m x, \u001b[38;5;28mself\u001b[39m.activation\n\u001b[32m    143\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_linear.py:97\u001b[39m, in \u001b[36mLinear.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m     96\u001b[39m     x = jnp.broadcast_to(x, (\u001b[32m1\u001b[39m,))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:1141\u001b[39m, in \u001b[36m_forward_operator_to_aval.<locals>.op\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:604\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:263\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    267\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    268\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    269\u001b[39m     const_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:136\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_python_pjit_helper\u001b[39m(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_flat:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:318\u001b[39m, in \u001b[36m_argnames_partial\u001b[39m\u001b[34m(_fun, _fixed_kwargs, *args, **dyn_kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m({k: v.val \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _fixed_kwargs.val.items()}, **dyn_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\tensor_contractions.py:254\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m    253\u001b[39m b = lax.squeeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m out = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m result = lax.transpose(out, perm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:2448\u001b[39m, in \u001b[36mdot_general\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   2442\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.lax.dot`.\u001b[39;00m\n\u001b[32m   2443\u001b[39m \n\u001b[32m   2444\u001b[39m \u001b[33;03mPrefer use of :func:`jax.lax.dot` directly, but note that it requires\u001b[39;00m\n\u001b[32m   2445\u001b[39m \u001b[33;03mall arguments after ``lhs`` and ``rhs`` to be specified by keyword\u001b[39;00m\n\u001b[32m   2446\u001b[39m \u001b[33;03mrather than position.\u001b[39;00m\n\u001b[32m   2447\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2448\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2449\u001b[39m \u001b[43m           \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:2557\u001b[39m, in \u001b[36mdot\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2556\u001b[39m lhs, rhs = core.standard_insert_pvary(lhs, rhs)\n\u001b[32m-> \u001b[39m\u001b[32m2557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot_general_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2559\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcanonicalize_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2561\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:632\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    631\u001b[39m args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:648\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:660\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    659\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2175\u001b[39m, in \u001b[36mDynamicJaxprTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, tracers, params)\u001b[39m\n\u001b[32m   2173\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m custom_staging_rules[primitive](\u001b[38;5;28mself\u001b[39m, source_info, *jaxpr_tracers,\n\u001b[32m   2174\u001b[39m                                          **params)\n\u001b[32m-> \u001b[39m\u001b[32m2175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_process_primitive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr_tracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2189\u001b[39m, in \u001b[36mDynamicJaxprTrace.default_process_primitive\u001b[39m\u001b[34m(self, primitive, tracers, params, source_info)\u001b[39m\n\u001b[32m   2188\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2189\u001b[39m   out_avals, effs = \u001b[43m_cached_abstract_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43maval_qdds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2191\u001b[39m   \u001b[38;5;66;03m# TODO(phawkins): remove this 3 months after the release of JAX v0.7.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\util.py:460\u001b[39m, in \u001b[36mmulti_weakref_lru_cache.<locals>.wrapper\u001b[39m\u001b[34m(*orig_args, **orig_kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nr_weakrefs == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_multi_weakref_placeholder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nr_weakrefs == \u001b[32m1\u001b[39m:\n\u001b[32m    463\u001b[39m   \u001b[38;5;66;03m# Put the single weakref first, and skip the MultiWeakRefCacheKey\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\util.py:444\u001b[39m, in \u001b[36mmulti_weakref_lru_cache.<locals>.cache_miss\u001b[39m\u001b[34m(key, *args, **kwargs)\u001b[39m\n\u001b[32m    442\u001b[39m   orig_args, orig_kwargs = sentinel_to_referrents(\n\u001b[32m    443\u001b[39m       (args, kwargs), \u001b[38;5;28miter\u001b[39m([weakref.ref(key)]), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2004\u001b[39m, in \u001b[36m_cached_abstract_eval\u001b[39m\u001b[34m(primitive, *aval_qdds, **params)\u001b[39m\n\u001b[32m   2002\u001b[39m \u001b[38;5;129m@multi_weakref_lru_cache\u001b[39m\n\u001b[32m   2003\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_cached_abstract_eval\u001b[39m(primitive: core.Primitive, *aval_qdds, **params):\n\u001b[32m-> \u001b[39m\u001b[32m2004\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabstract_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43maval_qdds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:702\u001b[39m, in \u001b[36m_effect_free_abstract_eval.<locals>.abstract_eval_\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mabstract_eval_\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mabstract_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, no_effects\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\utils.py:168\u001b[39m, in \u001b[36mstandard_abstract_eval\u001b[39m\u001b[34m(prim, shape_rule, dtype_rule, weak_type_rule, sharding_rule, vma_rule, unreduced_rule, memory_space_rule, *avals, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m core.check_avals_context_mesh(avals, prim.name)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m out_shape, out_dtype, out_sharding = \u001b[43mcall_shape_dtype_sharding_rule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munreduced_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m out_vma = vma_rule(*avals, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\utils.py:111\u001b[39m, in \u001b[36mcall_shape_dtype_sharding_rule\u001b[39m\u001b[34m(prim, shape_rule, dtype_rule, sharding_rule, unreduced_rule, multi_out, *avals, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcall_shape_dtype_sharding_rule\u001b[39m(prim, shape_rule, dtype_rule, sharding_rule,\n\u001b[32m    110\u001b[39m                                    unreduced_rule, multi_out, *avals, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m   out_shapes = \u001b[43mshape_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m   out_dtypes = dtype_rule(*avals, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:5303\u001b[39m, in \u001b[36m_dot_general_shape_rule\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   5301\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5302\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5303\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[32m   5305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
      "\u001b[31mTypeError\u001b[39m: dot_general requires contracting dimensions to have the same shape, got (65,) and (128,).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:200\u001b[39m, in \u001b[36m_assert_term_compatible\u001b[39m\u001b[34m(t, y, args, terms, term_structure, contr_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m jax.numpy_dtype_promotion(\u001b[33m\"\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         \u001b[43mjtu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontr_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# ValueError may also arise from mismatched tree structures\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:364\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    363\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:364\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    363\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:172\u001b[39m, in \u001b[36m_assert_term_compatible.<locals>._check\u001b[39m\u001b[34m(term_cls, term, term_contr_kwargs, yi)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.vf: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m    173\u001b[39m vf_type_compatible = eqx.filter_eval_shape(\n\u001b[32m    174\u001b[39m     better_isinstance, vf_type, vf_type_expected\n\u001b[32m    175\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Error while tracing ODETerm(vector_field=<function integrate_batch.<locals>.rhs_flat>).vf: dot_general requires contracting dimensions to have the same shape, got (65,) and (128,).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 276\u001b[39m\n\u001b[32m    274\u001b[39m xb_j = jnp.array(xb)\n\u001b[32m    275\u001b[39m ab_j = jnp.array(ab)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m loss_val, grads = \u001b[43mloss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m model, opt_state = apply_updates(model, grads, opt_state)\n\u001b[32m    278\u001b[39m losses.append(\u001b[38;5;28mfloat\u001b[39m(loss_val))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 237\u001b[39m, in \u001b[36mloss_and_grad\u001b[39m\u001b[34m(model, x_batch, alpha_batch)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m l + l2\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# Berechne den Gradienten bezüglich der Parameter 'params'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m loss_val, grads = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_loss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss_val, grads\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_ad.py:71\u001b[39m, in \u001b[36m_ValueAndGradWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m x, *args = args\n\u001b[32m     70\u001b[39m diff_x, nondiff_x = partition(x, is_inexact_array)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnondiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 11 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 229\u001b[39m, in \u001b[36mloss_and_grad.<locals>._loss_fn\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_loss_fn\u001b[39m(p):\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# Übergebe die Parameter an sequence_loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     l, _ = \u001b[43msequence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# Weight decay\u001b[39;00m\n\u001b[32m    232\u001b[39m     leaves = jax.tree_util.tree_leaves(p)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36msequence_loss\u001b[39m\u001b[34m(params, model, x_seq, alpha_true)\u001b[39m\n\u001b[32m    209\u001b[39m h = jax.vmap(model.encoder)(x_seq[:, \u001b[32m0\u001b[39m, :])  \u001b[38;5;66;03m# (B, h_dim)\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T - \u001b[32m1\u001b[39m):\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# Übergebe die statische h_dim\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     h = \u001b[43mintegrate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43modefunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mh_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt_grid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt_grid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     h = model.rnn_update(x_seq[:, i+\u001b[32m1\u001b[39m, :], h)\n\u001b[32m    216\u001b[39m alpha_pred = model.decoder(h)  \u001b[38;5;66;03m# (B,1)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mintegrate_batch\u001b[39m\u001b[34m(ode_module, h_dim_static, h0, t0, t1)\u001b[39m\n\u001b[32m    171\u001b[39m term = diffrax.ODETerm(rhs_flat)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# (Entferne die try/except-Blöcke, die diffrax-Version sollte stabil sein)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m sol = \u001b[43mdiffrax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiffeqsolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m=\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Automatisch\u001b[39;49;00m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1_000_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstepsize_controller\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Verwende den globalen Controller\u001b[39;49;00m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m ys = sol.ys\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:1103\u001b[39m, in \u001b[36mdiffeqsolve\u001b[39m\u001b[34m(terms, solver, t0, t1, dt0, y0, args, saveat, stepsize_controller, adjoint, event, max_steps, throw, progress_meter, solver_state, controller_state, made_jump, discrete_terminating_event)\u001b[39m\n\u001b[32m   1100\u001b[39m         terms = MultiTerm(*terms)\n\u001b[32m   1102\u001b[39m \u001b[38;5;66;03m# Error checking for term compatibility\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m \u001b[43m_assert_term_compatible\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterm_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterm_compatible_contr_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sde(terms):\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(solver, (AbstractItoSolver, AbstractStratonovichSolver)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:205\u001b[39m, in \u001b[36m_assert_term_compatible\u001b[39m\u001b[34m(t, y, args, terms, term_structure, contr_kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m pretty_term = wl.pformat(terms)\n\u001b[32m    204\u001b[39m pretty_expected = wl.pformat(term_structure)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    206\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTerms are not compatible with solver! Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpretty_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbut expected:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpretty_expected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote that terms are checked recursively: if you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscroll up you may find a root-cause error that is more specific.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Terms are not compatible with solver! Got:\nODETerm(vector_field=<function integrate_batch.<locals>.rhs_flat>)\nbut expected:\ndiffrax.AbstractTerm\nNote that terms are checked recursively: if you scroll up you may find a root-cause error that is more specific."
     ]
    }
   ],
   "source": [
    "# ode_rnn_alpha_fixed.py\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train = data[\"xy_train\"].astype(np.float32)   # (N, T, 2)\n",
    "alpha_train = data[\"alpha_train\"].astype(np.float32)  # (N, 1)\n",
    "xy_test = data[\"xy_test\"].astype(np.float32)\n",
    "\n",
    "N, T, D = xy_train.shape\n",
    "assert D == 2\n",
    "\n",
    "# normalize inputs (per-dimension)\n",
    "xy_mean = xy_train.mean(axis=(0,1), keepdims=True)\n",
    "xy_std  = xy_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "xy_train_n = (xy_train - xy_mean) / xy_std\n",
    "xy_test_n  = (xy_test - xy_mean) / xy_std\n",
    "\n",
    "alpha_mean = alpha_train.mean(axis=0, keepdims=True)\n",
    "alpha_std  = alpha_train.std(axis=0, keepdims=True) + 1e-6\n",
    "alpha_train_n = (alpha_train - alpha_mean) / alpha_std\n",
    "\n",
    "# convert to jax\n",
    "xy_train_n = jnp.array(xy_train_n)\n",
    "alpha_train_n = jnp.array(alpha_train_n)\n",
    "xy_test_n = jnp.array(xy_test_n)\n",
    "\n",
    "# temporal grid: assume observations evenly spaced in [0,1]\n",
    "t_grid = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)\n",
    "\n",
    "# Model components (Equinox)\n",
    "\n",
    "# Cell In[43], line 50\n",
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        # KORRIGIERT: Entferne jnp.atleast_2d und die squeeze-Logik.\n",
    "        # MLP verarbeitet 1D-Vektoren (aus vmap) oder 2D-Batches (direkter Aufruf)\n",
    "        # korrekt, solange wir nicht versuchen, 1D zu 2D zu zwingen.\n",
    "        out = self.net(x0) # x0 ist entweder (D,) (innerhalb von vmap) oder (B, D)\n",
    "        return out                  # -> (B, out_size)\n",
    "# -----------------------------------------------------------------\n",
    "# KORRIGIERTE KLASSE: ODEFunc (zurück zum Original)\n",
    "# -----------------------------------------------------------------\n",
    "# Cell In[48], line 64\n",
    "# Cell In[...], ODEFunc Class\n",
    "class ODEFunc(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, h_dim, *, key):\n",
    "        # WICHTIG: in_size muss h_dim + 1 sein (für Zeit-Feature)\n",
    "        self.net = eqx.nn.MLP(\n",
    "            in_size=h_dim + 1,  # h_dim + 1 für Zeit\n",
    "            out_size=h_dim,\n",
    "            width_size=128,\n",
    "            depth=2,\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        # h: (B, h_dim) oder möglicherweise (h_dim,) während Tracing\n",
    "        B = h.shape[0] if h.ndim > 1 else 1\n",
    "        h = jnp.atleast_2d(h)  # Sicherstellen dass es 2D ist\n",
    "        \n",
    "        # Zeit-Feature erstellen und auf Batch-Size wiederholen\n",
    "        t_scalar = jnp.atleast_1d(t)[None, :]  # (1, 1)\n",
    "        t_feat = jnp.repeat(t_scalar, h.shape[0], axis=0)  # (B, 1)\n",
    "        \n",
    "        # Konkatenieren: h (B, h_dim) + t_feat (B, 1) -> (B, h_dim+1)\n",
    "        inp = jnp.concatenate([h, t_feat], axis=-1)\n",
    "        \n",
    "        # MLP durchlaufen\n",
    "        out = self.net(inp)  # (B, h_dim)\n",
    "        \n",
    "        return out\n",
    "class Decoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=64, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        h_mat = jnp.atleast_2d(h)  # (B, h_dim)\n",
    "        out = self.net(h_mat)      # -> (B, out_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNNUpdate(eqx.Module):\n",
    "    lin_x: eqx.nn.Linear\n",
    "    lin_h: eqx.nn.Linear\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, key):\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        self.lin_x = eqx.nn.Linear(x_dim, 3 * h_dim, key=k1)\n",
    "        self.lin_h = eqx.nn.Linear(h_dim, 3 * h_dim, key=k2)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        x = jnp.atleast_2d(x)  # (B, x_dim)\n",
    "        h = jnp.atleast_2d(h)  # (B, h_dim)\n",
    "        gates = self.lin_x(x) + self.lin_h(h)  # (B, 3*h_dim)\n",
    "        z, r, o = jnp.split(gates, 3, axis=-1)\n",
    "        z = jnn.sigmoid(z); r = jnn.sigmoid(r); o = jnp.tanh(o)\n",
    "        h_new = (1 - z) * h + z * o\n",
    "        return h_new\n",
    "\n",
    "class ODERNN(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    odefunc: ODEFunc\n",
    "    rnn_update: RNNUpdate\n",
    "    decoder: Decoder\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, *, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.encoder = Encoder(x_dim, h_dim, key=k1)\n",
    "        self.odefunc = ODEFunc(h_dim, key=k2)\n",
    "        self.rnn_update = RNNUpdate(x_dim, h_dim, key=k3)\n",
    "        self.decoder = Decoder(h_dim, 1, key=k4)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "# Solver and utils\n",
    "solver = diffrax.Tsit5()\n",
    "adjoint = diffrax.BacksolveAdjoint()\n",
    "controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# KORRIGIERTE FUNKTION: integrate_batch (JIT-sicher)\n",
    "# -----------------------------------------------------------------\n",
    "def integrate_batch(ode_module, h_dim_static, h0, t0, t1):\n",
    "    \"\"\"\n",
    "    ode_module: das eqx.Module (z.B. model.odefunc)\n",
    "    h_dim_static: die statische h_dim (z.B. 64)\n",
    "    h0: (B, h_dim)\n",
    "    \"\"\"\n",
    "    h0 = jnp.atleast_2d(h0)\n",
    "    y0 = h0.ravel()\n",
    "\n",
    "    # rhs liest odefunc aus args, kein closure\n",
    "    def rhs_flat(t, y, args):\n",
    "        odefunc_from_args = args[0]\n",
    "        \n",
    "        # 1. Sicherstellen, dass y die korrekte Hidden-Dimension verwendet\n",
    "        # y hat die Form (B * h_dim_static,)\n",
    "        y_mat = y.reshape((-1, h_dim_static)) \n",
    "        \n",
    "        # 2. ODEFunc aufrufen: (B, h_dim) -> (B, h_dim)\n",
    "        dydt = odefunc_from_args(t, y_mat, None) \n",
    "        \n",
    "        # 3. Rückgabe muss (B * h_dim,) sein\n",
    "        return dydt.ravel()\n",
    "\n",
    "    term = diffrax.ODETerm(rhs_flat)\n",
    "    \n",
    "    # (Entferne die try/except-Blöcke, die diffrax-Version sollte stabil sein)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        y0=y0,\n",
    "        args=(ode_module,),\n",
    "        dt0=None, # Automatisch\n",
    "        max_steps=1_000_000,\n",
    "        adjoint=adjoint,\n",
    "        stepsize_controller=controller, # Verwende den globalen Controller\n",
    "    )\n",
    "\n",
    "    ys = sol.ys\n",
    "    try:\n",
    "        y_final = ys[-1]\n",
    "    except Exception:\n",
    "        y_final = ys\n",
    "\n",
    "    # KORREKTUR: Leite 'B' aus y_final ab\n",
    "    h1 = jnp.reshape(y_final, (-1, h_dim_static))\n",
    "    return h1\n",
    "\n",
    "def mse(a, b):\n",
    "    return jnp.mean((a - b) ** 2)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# KORRIGIERTE FUNKTION: sequence_loss (zurück zum Original)\n",
    "# -----------------------------------------------------------------\n",
    "def sequence_loss(params, model: ODERNN, x_seq, alpha_true):\n",
    "    # x_seq: (B, T, D)\n",
    "    \n",
    "    # Ersetze die statischen Teile des Modells mit den trainierbaren Parametern\n",
    "    model = eqx.combine(model, params)\n",
    "    \n",
    "    h = jax.vmap(model.encoder)(x_seq[:, 0, :])  # (B, h_dim)\n",
    "    \n",
    "    for i in range(T - 1):\n",
    "        # Übergebe die statische h_dim\n",
    "        h = integrate_batch(model.odefunc, model.h_dim, h, float(t_grid[i]), float(t_grid[i+1]))\n",
    "        h = model.rnn_update(x_seq[:, i+1, :], h)\n",
    "        \n",
    "    alpha_pred = model.decoder(h)  # (B,1)\n",
    "    loss = mse(alpha_pred, alpha_true)\n",
    "    return loss, alpha_pred\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# KORRIGIERTE FUNKTION: loss_and_grad (zurück zum Original)\n",
    "# -----------------------------------------------------------------\n",
    "def loss_and_grad(model, x_batch, alpha_batch):\n",
    "    # Filtere nur die trainierbaren Parameter (Arrays) heraus\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    \n",
    "    def _loss_fn(p):\n",
    "        # Übergebe die Parameter an sequence_loss\n",
    "        l, _ = sequence_loss(p, model, x_batch, alpha_batch)\n",
    "        \n",
    "        # Weight decay\n",
    "        leaves = jax.tree_util.tree_leaves(p)\n",
    "        l2 = 1e-6 * sum(jnp.sum(p ** 2) for p in leaves)\n",
    "        return l + l2\n",
    "        \n",
    "    # Berechne den Gradienten bezüglich der Parameter 'params'\n",
    "    loss_val, grads = eqx.filter_value_and_grad(_loss_fn)(params)\n",
    "    return loss_val, grads\n",
    "\n",
    "# Training loop\n",
    "key = random.PRNGKey(0)\n",
    "h_dim = 64\n",
    "model = ODERNN(x_dim=D, h_dim=h_dim, key=key)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 5e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "def data_loader(xy, alpha, batch_size, shuffle=True):\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        b = idx[i:i+batch_size]\n",
    "        yield xy[b], alpha[b]\n",
    "\n",
    "@jax.jit\n",
    "def apply_updates(model, grads, opt_state):\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state\n",
    "\n",
    "# Training-Loop (unverändert)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    for xb, ab in data_loader(np.array(xy_train_n), np.array(alpha_train_n), batch_size):\n",
    "        xb_j = jnp.array(xb)\n",
    "        ab_j = jnp.array(ab)\n",
    "        loss_val, grads = loss_and_grad(model, xb_j, ab_j)\n",
    "        model, opt_state = apply_updates(model, grads, opt_state)\n",
    "        losses.append(float(loss_val))\n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} loss={np.mean(losses):.6f} time={t1-t0:.1f}s\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# KORRIGIERTE FUNKTION: predict_alpha (zurück zum Original)\n",
    "# -----------------------------------------------------------------\n",
    "def predict_alpha(model, xy_input):\n",
    "    # xy_input: (N, T, D) normalized\n",
    "    batch_size = 128\n",
    "    preds = []\n",
    "    for i in range(0, xy_input.shape[0], batch_size):\n",
    "        xb = xy_input[i:i+batch_size]\n",
    "        B = xb.shape[0]\n",
    "        h = jax.vmap(model.encoder)(xb[:, 0, :])\n",
    "        for j in range(T - 1):\n",
    "            # Übergebe die statische h_dim\n",
    "            h = integrate_batch(model.odefunc, model.h_dim, h, float(t_grid[j]), float(t_grid[j+1]))\n",
    "            h = model.rnn_update(xb[:, j+1, :], h)\n",
    "        alpha_p = model.decoder(h)  # (B,1) normalized\n",
    "        preds.append(np.array(alpha_p))\n",
    "    preds = np.vstack(preds)\n",
    "    # denormalize\n",
    "    preds_orig = preds * alpha_std + alpha_mean\n",
    "    return preds_orig\n",
    "\n",
    "alpha_test_pred = predict_alpha(model, xy_test_n)  # shape (N,1)\n",
    "np.save(\"alpha_test_pred.npy\", alpha_test_pred)\n",
    "print(\"Saved alpha_test_pred.npy with shape\", alpha_test_pred.shape)\n",
    "\n",
    "# optional quick eval on train set (to sanity-check) and plot\n",
    "subset = 200\n",
    "train_pred = predict_alpha(model, xy_train_n[:subset])\n",
    "train_true = np.array(alpha_train[:subset])\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(train_true.ravel(), train_pred.ravel(), alpha=0.6, s=20, label=\"pred vs true\")\n",
    "minv = min(train_true.min(), train_pred.min())\n",
    "maxv = max(train_true.max(), train_pred.max())\n",
    "plt.plot([minv, maxv], [minv, maxv], 'r--', label=\"identity\")\n",
    "plt.xlabel(\"alpha true\")\n",
    "plt.ylabel(\"alpha predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Predicted alpha vs True alpha (subset)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a349da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape float32[]\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\nThe error occurred while tracing the function update_step at c:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_jit.py:43 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:bool[128,128]\u001b[39m = gt b 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:f32[128]\u001b[39m = broadcast_in_dim[\n  broadcast_dimensions=()\n  shape=(128,)\n  sharding=None\n] 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:bool[128,128]\u001b[39m = gt b 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:f32[128]\u001b[39m = broadcast_in_dim[\n  broadcast_dimensions=()\n  shape=(128,)\n  sharding=None\n] 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:f32[1]\u001b[39m = slice[limit_indices=(1,) start_indices=(0,) strides=None] b\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:221:52 (sequence_loss)\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConcretizationTypeError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 305\u001b[39m\n\u001b[32m    302\u001b[39m     xb_j = jnp.array(xb)\n\u001b[32m    303\u001b[39m     ab_j = jnp.array(ab)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     model, opt_state, loss_val = \u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     losses.append(\u001b[38;5;28mfloat\u001b[39m(loss_val))\n\u001b[32m    308\u001b[39m t1 = time.time()\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mupdate_step\u001b[39m\u001b[34m(model, opt_state, x_batch, alpha_batch, optimizer)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;129m@eqx\u001b[39m.filter_jit\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mupdate_step\u001b[39m(model, opt_state, x_batch, alpha_batch, optimizer):\n\u001b[32m    255\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Single training step.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     loss_val, grads = \u001b[43mloss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# Get trainable parameters\u001b[39;00m\n\u001b[32m    259\u001b[39m     params = eqx.filter(model, eqx.is_array)\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 239\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(model, x_batch, alpha_batch)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mloss_fn\u001b[39m(model, x_batch, alpha_batch):\n\u001b[32m    238\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss with L2 regularization.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     loss, _ = \u001b[43msequence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Add L2 regularization\u001b[39;00m\n\u001b[32m    242\u001b[39m     params = eqx.filter(model, eqx.is_array)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36msequence_loss\u001b[39m\u001b[34m(model, x_seq, alpha_true)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Process sequence\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T - \u001b[32m1\u001b[39m):\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# Integrate hidden state through time\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     h = integrate_batch(model.odefunc, h, \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt_grid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(t_grid[i+\u001b[32m1\u001b[39m]))\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# Update with next observation\u001b[39;00m\n\u001b[32m    223\u001b[39m     h = model.rnn_update(x_seq[:, i+\u001b[32m1\u001b[39m, :], h)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:1811\u001b[39m, in \u001b[36mconcretization_function_error.<locals>.error\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[31mConcretizationTypeError\u001b[39m: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[]\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\nThe error occurred while tracing the function update_step at c:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_jit.py:43 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:bool[128,128]\u001b[39m = gt b 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:f32[128]\u001b[39m = broadcast_in_dim[\n  broadcast_dimensions=()\n  shape=(128,)\n  sharding=None\n] 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:bool[128,128]\u001b[39m = gt b 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:f32[128]\u001b[39m = broadcast_in_dim[\n  broadcast_dimensions=()\n  shape=(128,)\n  sharding=None\n] 0.0:f32[]\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:53:15 (Encoder.__call__)\n\n  operation a\u001b[35m:f32[1]\u001b[39m = slice[limit_indices=(1,) start_indices=(0,) strides=None] b\n    from line C:\\Users\\Nilsl\\AppData\\Local\\Temp\\ipykernel_22096\\3542125018.py:221:52 (sequence_loss)\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "# ode_rnn_spiral_fixed.py\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train = data[\"xy_train\"].astype(np.float32)   # (N, T, 2)\n",
    "alpha_train = data[\"alpha_train\"].astype(np.float32)  # (N, 1)\n",
    "xy_test = data[\"xy_test\"].astype(np.float32)\n",
    "\n",
    "N, T, D = xy_train.shape\n",
    "assert D == 2\n",
    "\n",
    "# normalize inputs (per-dimension)\n",
    "xy_mean = xy_train.mean(axis=(0,1), keepdims=True)\n",
    "xy_std  = xy_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "xy_train_n = (xy_train - xy_mean) / xy_std\n",
    "xy_test_n  = (xy_test - xy_mean) / xy_std\n",
    "\n",
    "alpha_mean = alpha_train.mean(axis=0, keepdims=True)\n",
    "alpha_std  = alpha_train.std(axis=0, keepdims=True) + 1e-6\n",
    "alpha_train_n = (alpha_train - alpha_mean) / alpha_std\n",
    "\n",
    "# convert to jax\n",
    "xy_train_n = jnp.array(xy_train_n)\n",
    "alpha_train_n = jnp.array(alpha_train_n)\n",
    "xy_test_n = jnp.array(xy_test_n)\n",
    "\n",
    "# temporal grid: assume observations evenly spaced in [0,1]\n",
    "t_grid = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Model components (Equinox)\n",
    "# -----------------------------\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        return self.net(x0)\n",
    "\n",
    "#locate concat error in ODEFunc\n",
    "# Cell In[48], line 64\n",
    "#\n",
    "class ODEFunc(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, h_dim, *, key):\n",
    "        self.net = eqx.nn.MLP(\n",
    "            in_size=h_dim + 1,  # h_dim + 1 for time feature\n",
    "            out_size=h_dim,\n",
    "            width_size=128,\n",
    "            depth=2,\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        # h: (h_dim,) for single sample or (B, h_dim) for batch\n",
    "        h = jnp.atleast_2d(h)  # ensure 2D\n",
    "        B = h.shape[0]\n",
    "        \n",
    "        # Create time feature\n",
    "        t_feat = jnp.full((B, 1), t, dtype=h.dtype)\n",
    "        \n",
    "        # Concatenate: h (B, h_dim) + t_feat (B, 1) -> (B, h_dim+1)\n",
    "        inp = jnp.concatenate([h, t_feat], axis=-1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        out = self.net(inp)  # (B, h_dim)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=64, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        h_mat = jnp.atleast_2d(h)  # (B, h_dim)\n",
    "        out = self.net(h_mat)      # -> (B, out_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNNUpdate(eqx.Module):\n",
    "    lin_x: eqx.nn.Linear\n",
    "    lin_h: eqx.nn.Linear\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, key):\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        self.lin_x = eqx.nn.Linear(x_dim, 3 * h_dim, key=k1)\n",
    "        self.lin_h = eqx.nn.Linear(h_dim, 3 * h_dim, key=k2)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        x = jnp.atleast_2d(x)  # (B, x_dim)\n",
    "        h = jnp.atleast_2d(h)  # (B, h_dim)\n",
    "        gates = self.lin_x(x) + self.lin_h(h)  # (B, 3*h_dim)\n",
    "        z, r, o = jnp.split(gates, 3, axis=-1)\n",
    "        z = jnn.sigmoid(z)\n",
    "        r = jnn.sigmoid(r)\n",
    "        o = jnp.tanh(o)\n",
    "        h_new = (1 - z) * h + z * o\n",
    "        return h_new\n",
    "\n",
    "\n",
    "class ODERNN(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    odefunc: ODEFunc\n",
    "    rnn_update: RNNUpdate\n",
    "    decoder: Decoder\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, *, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.encoder = Encoder(x_dim, h_dim, key=k1)\n",
    "        self.odefunc = ODEFunc(h_dim, key=k2)\n",
    "        self.rnn_update = RNNUpdate(x_dim, h_dim, key=k3)\n",
    "        self.decoder = Decoder(h_dim, 1, key=k4)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "\n",
    "# Solver configuration\n",
    "solver = diffrax.Tsit5()\n",
    "adjoint = diffrax.BacksolveAdjoint()\n",
    "controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FIXED: integrate_batch\n",
    "# -----------------------------\n",
    "def integrate_batch(odefunc, h0, t0, t1):\n",
    "    \"\"\"\n",
    "    Integrate hidden state from t0 to t1 using the ODE function.\n",
    "    \n",
    "    Args:\n",
    "        odefunc: ODEFunc module (must be passed directly, not through args)\n",
    "        h0: Initial hidden state (B, h_dim)\n",
    "        t0: Start time\n",
    "        t1: End time\n",
    "    \n",
    "    Returns:\n",
    "        h1: Final hidden state (B, h_dim)\n",
    "    \"\"\"\n",
    "    h0 = jnp.atleast_2d(h0)\n",
    "    B, h_dim = h0.shape\n",
    "    y0 = h0.ravel()\n",
    "    \n",
    "    # Define RHS that uses odefunc directly (not through args)\n",
    "    def rhs_flat(t, y, args):\n",
    "        y_mat = y.reshape((B, h_dim))\n",
    "        dydt = odefunc(t, y_mat, None)\n",
    "        return dydt.ravel()\n",
    "    \n",
    "    term = diffrax.ODETerm(rhs_flat)\n",
    "    \n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        y0=y0,\n",
    "        args=None,  # No args needed since odefunc is in closure\n",
    "        dt0=None,\n",
    "        max_steps=1_000_000,\n",
    "        adjoint=adjoint,\n",
    "        stepsize_controller=controller,\n",
    "        saveat=diffrax.SaveAt(t1=True),\n",
    "    )\n",
    "    \n",
    "    # Get final state\n",
    "    y_final = sol.ys[0] if hasattr(sol.ys, '__getitem__') else sol.ys\n",
    "    h1 = y_final.reshape((B, h_dim))\n",
    "    \n",
    "    return h1\n",
    "\n",
    "\n",
    "def mse(a, b):\n",
    "    return jnp.mean((a - b) ** 2)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FIXED: sequence_loss\n",
    "# -----------------------------\n",
    "def sequence_loss(model: ODERNN, x_seq, alpha_true):\n",
    "    \"\"\"\n",
    "    Compute loss for a batch of sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: ODERNN model\n",
    "        x_seq: Input sequences (B, T, D)\n",
    "        alpha_true: True alpha values (B, 1)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss\n",
    "        alpha_pred: Predicted alpha values (B, 1)\n",
    "    \"\"\"\n",
    "    B, T_seq, D = x_seq.shape\n",
    "    \n",
    "    # Encode first observation\n",
    "    h = jax.vmap(model.encoder)(x_seq[:, 0, :])  # (B, h_dim)\n",
    "    \n",
    "    # Process sequence\n",
    "    for i in range(T - 1):\n",
    "        # Integrate hidden state through time\n",
    "        h = integrate_batch(model.odefunc, h, float(t_grid[i]), float(t_grid[i+1]))\n",
    "        # Update with next observation\n",
    "        h = model.rnn_update(x_seq[:, i+1, :], h)\n",
    "    \n",
    "    # Decode to predict alpha\n",
    "    alpha_pred = model.decoder(h)  # (B, 1)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mse(alpha_pred, alpha_true)\n",
    "    \n",
    "    return loss, alpha_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training utilities\n",
    "# -----------------------------\n",
    "def loss_fn(model, x_batch, alpha_batch):\n",
    "    \"\"\"Compute loss with L2 regularization.\"\"\"\n",
    "    loss, _ = sequence_loss(model, x_batch, alpha_batch)\n",
    "    \n",
    "    # Add L2 regularization\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    leaves = jax.tree_util.tree_leaves(params)\n",
    "    l2 = 1e-6 * sum(jnp.sum(p ** 2) for p in leaves)\n",
    "    \n",
    "    return loss + l2\n",
    "\n",
    "\n",
    "# Create value_and_grad function\n",
    "loss_and_grad = eqx.filter_value_and_grad(loss_fn)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update_step(model, opt_state, x_batch, alpha_batch, optimizer):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    loss_val, grads = loss_and_grad(model, x_batch, alpha_batch)\n",
    "    \n",
    "    # Get trainable parameters\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    \n",
    "    # Update parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    \n",
    "    return model, opt_state, loss_val\n",
    "\n",
    "\n",
    "def data_loader(xy, alpha, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\"\"\"\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        b = idx[i:i+batch_size]\n",
    "        yield xy[b], alpha[b]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize model and optimizer\n",
    "# -----------------------------\n",
    "key = random.PRNGKey(0)\n",
    "h_dim = 64\n",
    "model = ODERNN(x_dim=D, h_dim=h_dim, key=key)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    \n",
    "    for xb, ab in data_loader(np.array(xy_train_n), np.array(alpha_train_n), batch_size):\n",
    "        xb_j = jnp.array(xb)\n",
    "        ab_j = jnp.array(ab)\n",
    "        \n",
    "        model, opt_state, loss_val = update_step(model, opt_state, xb_j, ab_j, optimizer)\n",
    "        losses.append(float(loss_val))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} loss={np.mean(losses):.6f} time={t1-t0:.1f}s\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction function\n",
    "# -----------------------------\n",
    "def predict_alpha(model, xy_input):\n",
    "    \"\"\"\n",
    "    Predict alpha values for input sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ODERNN model\n",
    "        xy_input: Input sequences (N, T, D) - normalized\n",
    "    \n",
    "    Returns:\n",
    "        alpha_pred: Predicted alpha values (N, 1) - denormalized\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, xy_input.shape[0], batch_size):\n",
    "        xb = xy_input[i:i+batch_size]\n",
    "        B = xb.shape[0]\n",
    "        \n",
    "        # Encode first observation\n",
    "        h = jax.vmap(model.encoder)(xb[:, 0, :])\n",
    "        \n",
    "        # Process sequence\n",
    "        for j in range(T - 1):\n",
    "            h = integrate_batch(model.odefunc, h, float(t_grid[j]), float(t_grid[j+1]))\n",
    "            h = model.rnn_update(xb[:, j+1, :], h)\n",
    "        \n",
    "        # Decode to predict alpha\n",
    "        alpha_p = model.decoder(h)  # (B, 1) normalized\n",
    "        preds.append(np.array(alpha_p))\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    \n",
    "    # Denormalize\n",
    "    preds_orig = preds * alpha_std + alpha_mean\n",
    "    \n",
    "    return preds_orig\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate predictions and save\n",
    "# -----------------------------\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "alpha_test_pred = predict_alpha(model, xy_test_n)\n",
    "np.save(\"alpha_test_pred.npy\", alpha_test_pred)\n",
    "print(f\"Saved alpha_test_pred.npy with shape {alpha_test_pred.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on training subset\n",
    "# -----------------------------\n",
    "print(\"\\nEvaluating on training subset...\")\n",
    "subset = 200\n",
    "train_pred = predict_alpha(model, xy_train_n[:subset])\n",
    "train_true = np.array(alpha_train[:subset])\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(train_true.ravel(), train_pred.ravel(), alpha=0.6, s=20, label=\"Predicted vs True\")\n",
    "minv = min(train_true.min(), train_pred.min())\n",
    "maxv = max(train_true.max(), train_pred.max())\n",
    "plt.plot([minv, maxv], [minv, maxv], 'r--', linewidth=2, label=\"Identity line\")\n",
    "plt.xlabel(\"Alpha (True)\", fontsize=12)\n",
    "plt.ylabel(\"Alpha (Predicted)\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.title(\"Predicted Alpha vs True Alpha (Training Subset)\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print metrics\n",
    "mae = np.mean(np.abs(train_true - train_pred))\n",
    "rmse = np.sqrt(np.mean((train_true - train_pred) ** 2))\n",
    "print(f\"\\nTraining subset metrics:\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "#fix error concat type error in ODEFunc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b19e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Terms are not compatible with solver! Got:\nODETerm(vector_field=<function _rhs_single>)\nbut expected:\ndiffrax.AbstractTerm\nNote that terms are checked recursively: if you scroll up you may find a root-cause error that is more specific.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:170\u001b[39m, in \u001b[36m_assert_term_compatible.<locals>._check\u001b[39m\u001b[34m(term_cls, term, term_contr_kwargs, yi)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     vf_type = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_eval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_eval_shape.py:38\u001b[39m, in \u001b[36mfilter_eval_shape\u001b[39m\u001b[34m(fun, *args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m dynamic, static = partition((fun, args, kwargs), _filter)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m dynamic_out, static_out = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combine(dynamic_out, static_out.value)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api.py:3062\u001b[39m, in \u001b[36meval_shape\u001b[39m\u001b[34m(fun, *args, **kwargs)\u001b[39m\n\u001b[32m   3061\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m: fun = partial(fun)\n\u001b[32m-> \u001b[39m\u001b[32m3062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.out_info\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:305\u001b[39m, in \u001b[36mjit_trace\u001b[39m\u001b[34m(jit_func, *args, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mjit_trace\u001b[39m(jit_func, *args, **kwargs) -> stages.Traced:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjit_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m   lower_callable = partial(_resolve_and_lower, args_flat, pgle_profiler=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_eval_shape.py:33\u001b[39m, in \u001b[36mfilter_eval_shape.<locals>._fn\u001b[39m\u001b[34m(_static, _dynamic)\u001b[39m\n\u001b[32m     32\u001b[39m _fun, _args, _kwargs = combine(_static, _dynamic)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m _out = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m _dynamic_out, _static_out = partition(_out, _filter)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_term.py:194\u001b[39m, in \u001b[36mODETerm.vf\u001b[39m\u001b[34m(self, t, y, args)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mvf\u001b[39m(\u001b[38;5;28mself\u001b[39m, t: RealScalarLike, y: Y, args: Args) -> _VF:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m jtu.tree_structure(out) != jtu.tree_structure(y):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36m_rhs_single\u001b[39m\u001b[34m(t, y, args)\u001b[39m\n\u001b[32m    172\u001b[39m h_mat = h[\u001b[38;5;28;01mNone\u001b[39;00m, :]                 \u001b[38;5;66;03m# (1, h_dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m dh = \u001b[43mode_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# should return (1, h_dim)\u001b[39;00m\n\u001b[32m    174\u001b[39m dh = jnp.asarray(dh).reshape(-\u001b[32m1\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:263\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    267\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    268\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    269\u001b[39m     const_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:136\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_python_pjit_helper\u001b[39m(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_flat:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:318\u001b[39m, in \u001b[36m_argnames_partial\u001b[39m\u001b[34m(_fun, _fixed_kwargs, *args, **dyn_kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m({k: v.val \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _fixed_kwargs.val.items()}, **dyn_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:292\u001b[39m, in \u001b[36m_argnums_partial\u001b[39m\u001b[34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(fixed_args_, sentinel) \u001b[38;5;129;01mis\u001b[39;00m sentinel\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_jit.py:55\u001b[39m, in \u001b[36m_filter_jit_cache.<locals>.fun_wrapped\u001b[39m\u001b[34m(dynamic_donate, dynamic_nodonate, static)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dummy_arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m out = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m dynamic_out, static_out = partition(out, is_array)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mmake_odefunc_callable.<locals>._callable\u001b[39m\u001b[34m(t, h, args_unused)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;129m@eqx\u001b[39m.filter_jit\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_callable\u001b[39m(t, h, args_unused):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mode_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mODEFunc.__call__\u001b[39m\u001b[34m(self, t, h, args)\u001b[39m\n\u001b[32m     74\u001b[39m original_shape = h.shape\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m h_mat = \u001b[43mensure_batched\u001b[49m(h)      \u001b[38;5;66;03m# (B, h_dim)\u001b[39;00m\n\u001b[32m     76\u001b[39m B = h_mat.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'ensure_batched' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:200\u001b[39m, in \u001b[36m_assert_term_compatible\u001b[39m\u001b[34m(t, y, args, terms, term_structure, contr_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m jax.numpy_dtype_promotion(\u001b[33m\"\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         \u001b[43mjtu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontr_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# ValueError may also arise from mismatched tree structures\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:364\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    363\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:364\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    363\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:172\u001b[39m, in \u001b[36m_assert_term_compatible.<locals>._check\u001b[39m\u001b[34m(term_cls, term, term_contr_kwargs, yi)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.vf: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m    173\u001b[39m vf_type_compatible = eqx.filter_eval_shape(\n\u001b[32m    174\u001b[39m     better_isinstance, vf_type, vf_type_expected\n\u001b[32m    175\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Error while tracing ODETerm(vector_field=<function _rhs_single>).vf: name 'ensure_batched' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 338\u001b[39m\n\u001b[32m    335\u001b[39m ab_j = jnp.array(ab)\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# Compute loss and gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m loss_val, grads = \u001b[43mcompute_loss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# Apply updates\u001b[39;00m\n\u001b[32m    341\u001b[39m model, opt_state = apply_updates(model, grads, opt_state, optimizer)\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 287\u001b[39m, in \u001b[36mcompute_loss_and_grad\u001b[39m\u001b[34m(model, x_batch, alpha_batch, t_grid_static)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;129m@eqx\u001b[39m.filter_jit\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcompute_loss_and_grad\u001b[39m(model, x_batch, alpha_batch, t_grid_static):\n\u001b[32m    286\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss and gradients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     loss_val, grads = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_grid_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_val, grads\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(model, x_batch, alpha_batch, t_grid_static)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mloss_fn\u001b[39m(model, x_batch, alpha_batch, t_grid_static):\n\u001b[32m    272\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss with L2 regularization.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loss, _ = \u001b[43msequence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_grid_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Add L2 regularization\u001b[39;00m\n\u001b[32m    276\u001b[39m     params = eqx.filter(model, eqx.is_array)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36msequence_loss\u001b[39m\u001b[34m(model, x_seq, alpha_true, t_grid_static)\u001b[39m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h_new\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Run loop from 0 to T-1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m h_final = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfori_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Decode to predict alpha\u001b[39;00m\n\u001b[32m    259\u001b[39m alpha_pred = model.decoder(h_final)\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36msequence_loss.<locals>.step_fn\u001b[39m\u001b[34m(i, h)\u001b[39m\n\u001b[32m    248\u001b[39m t0 = t_grid_static[i]\n\u001b[32m    249\u001b[39m t1 = t_grid_static[i + \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m h_ode = \u001b[43mintegrate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43modefunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Update with next observation\u001b[39;00m\n\u001b[32m    252\u001b[39m h_new = model.rnn_update(x_seq[:, i + \u001b[32m1\u001b[39m, :], h_ode)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 214\u001b[39m, in \u001b[36mintegrate_batch\u001b[39m\u001b[34m(ode_module, h0, t0, t1)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# vmap over first axis (batch)\u001b[39;00m\n\u001b[32m    213\u001b[39m integrate_vmapped = jax.vmap(\u001b[38;5;28;01mlambda\u001b[39;00m h: _integrate_single(ode_module, h, t0, t1))\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mintegrate_vmapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36mintegrate_batch.<locals>.<lambda>\u001b[39m\u001b[34m(h)\u001b[39m\n\u001b[32m    211\u001b[39m h0 = jnp.atleast_2d(h0)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# vmap over first axis (batch)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m integrate_vmapped = jax.vmap(\u001b[38;5;28;01mlambda\u001b[39;00m h: \u001b[43m_integrate_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m integrate_vmapped(h0)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36m_integrate_single\u001b[39m\u001b[34m(ode_module, h0_single, t0, t1)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Use the tracer-friendly wrapper _rhs_single which expects ode_callable passed in args\u001b[39;00m\n\u001b[32m    186\u001b[39m term = diffrax.ODETerm(vector_field=_rhs_single)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m sol = \u001b[43mdiffrax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiffeqsolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh0_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# pass the jittable callable as single-element tuple\u001b[39;49;00m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1_000_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstepsize_controller\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaveat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffrax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSaveAt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m y_final = sol.ys[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sol.ys, \u001b[33m\"\u001b[39m\u001b[33m__getitem__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m sol.ys\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jnp.asarray(y_final).reshape((\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)).squeeze(axis=\u001b[32m0\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:1103\u001b[39m, in \u001b[36mdiffeqsolve\u001b[39m\u001b[34m(terms, solver, t0, t1, dt0, y0, args, saveat, stepsize_controller, adjoint, event, max_steps, throw, progress_meter, solver_state, controller_state, made_jump, discrete_terminating_event)\u001b[39m\n\u001b[32m   1100\u001b[39m         terms = MultiTerm(*terms)\n\u001b[32m   1102\u001b[39m \u001b[38;5;66;03m# Error checking for term compatibility\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m \u001b[43m_assert_term_compatible\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterm_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterm_compatible_contr_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sde(terms):\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(solver, (AbstractItoSolver, AbstractStratonovichSolver)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:205\u001b[39m, in \u001b[36m_assert_term_compatible\u001b[39m\u001b[34m(t, y, args, terms, term_structure, contr_kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m pretty_term = wl.pformat(terms)\n\u001b[32m    204\u001b[39m pretty_expected = wl.pformat(term_structure)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    206\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTerms are not compatible with solver! Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpretty_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbut expected:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpretty_expected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote that terms are checked recursively: if you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscroll up you may find a root-cause error that is more specific.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Terms are not compatible with solver! Got:\nODETerm(vector_field=<function _rhs_single>)\nbut expected:\ndiffrax.AbstractTerm\nNote that terms are checked recursively: if you scroll up you may find a root-cause error that is more specific."
     ]
    }
   ],
   "source": [
    "# ode_rnn_spiral_fixed.py\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train = data[\"xy_train\"].astype(np.float32)   # (N, T, 2)\n",
    "alpha_train = data[\"alpha_train\"].astype(np.float32)  # (N, 1)\n",
    "xy_test = data[\"xy_test\"].astype(np.float32)\n",
    "\n",
    "N, T, D = xy_train.shape\n",
    "assert D == 2\n",
    "\n",
    "# normalize inputs (per-dimension)\n",
    "xy_mean = xy_train.mean(axis=(0,1), keepdims=True)\n",
    "xy_std  = xy_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "xy_train_n = (xy_train - xy_mean) / xy_std\n",
    "xy_test_n  = (xy_test - xy_mean) / xy_std\n",
    "\n",
    "alpha_mean = alpha_train.mean(axis=0, keepdims=True)\n",
    "alpha_std  = alpha_train.std(axis=0, keepdims=True) + 1e-6\n",
    "alpha_train_n = (alpha_train - alpha_mean) / alpha_std\n",
    "\n",
    "# convert to jax\n",
    "xy_train_n = jnp.array(xy_train_n)\n",
    "alpha_train_n = jnp.array(alpha_train_n)\n",
    "xy_test_n = jnp.array(xy_test_n)\n",
    "\n",
    "# temporal grid: assume observations evenly spaced in [0,1]\n",
    "t_grid = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Model components (Equinox)\n",
    "# -----------------------------\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        return self.net(x0)\n",
    "\n",
    "\n",
    "class ODEFunc(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, h_dim, *, key):\n",
    "        self.net = eqx.nn.MLP(\n",
    "            in_size=h_dim + 1,  # h_dim + 1 for time feature\n",
    "            out_size=h_dim,\n",
    "            width_size=128,\n",
    "            depth=2,\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        # h: Input state (potentially 1D or 2D)\n",
    "        # We need to ensure that the output shape matches the input shape,\n",
    "        # but the internal logic requires a BATCH dimension.\n",
    "        \n",
    "        # Store original shape for output\n",
    "        original_shape = h.shape\n",
    "        h_mat = ensure_batched(h)      # (B, h_dim)\n",
    "        B = h_mat.shape[0]\n",
    "        t_feat = jnp.full((B, 1), jnp.asarray(t, dtype=h_mat.dtype))\n",
    "        inp = jnp.concatenate([h_mat, t_feat], axis=-1)\n",
    "        out = self.net(inp)            # (B, h_dim)\n",
    "        # return with same rank as input\n",
    "        if len(original_shape) == 1:\n",
    "            return out.reshape((-1,))\n",
    "        return out.reshape(original_shape)\n",
    "\n",
    "\n",
    "class Decoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=64, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        h_mat = ensure_batched(h)\n",
    "        out = self.net(h_mat)\n",
    "        # keep batch semantics: if input was 1D return (out_dim,) else (B, out_dim)\n",
    "        if h.ndim == 1:\n",
    "            return out.reshape((-1,))  # (out_dim,)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNNUpdate(eqx.Module):\n",
    "    lin_x: eqx.nn.Linear\n",
    "    lin_h: eqx.nn.Linear\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, key):\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        self.lin_x = eqx.nn.Linear(x_dim, 3 * h_dim, key=k1)\n",
    "        self.lin_h = eqx.nn.Linear(h_dim, 3 * h_dim, key=k2)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        x = ensure_batched(x)\n",
    "        h = ensure_batched(h)\n",
    "        gates = self.lin_x(x) + self.lin_h(h)  # (B, 3*h_dim)\n",
    "        z, r, o = jnp.split(gates, 3, axis=-1)\n",
    "        z = jnn.sigmoid(z)\n",
    "        r = jnn.sigmoid(r)\n",
    "        o = jnp.tanh(o)\n",
    "        h_new = (1 - z) * h + z * o\n",
    "        return h_new\n",
    "\n",
    "\n",
    "class ODERNN(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    odefunc: ODEFunc\n",
    "    rnn_update: RNNUpdate\n",
    "    decoder: Decoder\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, *, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.encoder = Encoder(x_dim, h_dim, key=k1)\n",
    "        self.odefunc = ODEFunc(h_dim, key=k2)\n",
    "        self.rnn_update = RNNUpdate(x_dim, h_dim, key=k3)\n",
    "        self.decoder = Decoder(h_dim, 1, key=k4)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "\n",
    "# Solver configuration\n",
    "solver = diffrax.Tsit5()\n",
    "adjoint = diffrax.BacksolveAdjoint()\n",
    "controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
    "# Erstelle eine jittbare wrapper-Funktion für das Equinox ODE-Modul.\n",
    "# Das macht das Modul in JAX-Tracing stabil verwendbar.\n",
    "def make_odefunc_callable(ode_module):\n",
    "    # eqx.filter_jit macht eine JIT-geeignete, tracbare Callable aus einem PyTree-gebundenen Funktions-Closure.\n",
    "    # Sie gibt eine Funktion f(t, h, args_unused) zurück, die direkt das Equinox-Modul aufruft.\n",
    "    @eqx.filter_jit\n",
    "    def _callable(t, h, args_unused):\n",
    "        return ode_module(t, h, None)\n",
    "    return _callable\n",
    "\n",
    "# Top-level RHS, erwartete Signatur für diffrax: rhs(t, y, args)\n",
    "def _rhs_flat_top_level(t, y, args):\n",
    "    \"\"\"\n",
    "    args is a tuple (ode_callable, B, h_dim)\n",
    "    y is flat vector length B*h_dim\n",
    "    \"\"\"\n",
    "    ode_callable, B_local, h_dim_local = args\n",
    "    y_mat = y.reshape((B_local, h_dim_local))\n",
    "    dydt = ode_callable(t, y_mat, None)\n",
    "    return dydt.ravel()\n",
    "\n",
    "# Top-level single-sample RHS: tracerfreundlich und einfach\n",
    "def _rhs_single(t, y, args):\n",
    "    # args: (ode_callable,)\n",
    "    ode_callable, = args\n",
    "    h = jnp.asarray(y).reshape(-1)     # (h_dim,)\n",
    "    h_mat = h[None, :]                 # (1, h_dim)\n",
    "    dh = ode_callable(t, h_mat, None)  # should return (1, h_dim)\n",
    "    dh = jnp.asarray(dh).reshape(-1)\n",
    "    return dh\n",
    "\n",
    "\n",
    "def _integrate_single(ode_module, h0_single, t0, t1):\n",
    "    # Ensure input shape is 1D array\n",
    "    h0_vec = jnp.asarray(h0_single).reshape(-1)\n",
    "\n",
    "    # Make a jittable callable from the Equinox module before integrating\n",
    "    ode_callable = make_odefunc_callable(ode_module)  # returns f(t, h, args_unused)\n",
    "\n",
    "    # Use the tracer-friendly wrapper _rhs_single which expects ode_callable passed in args\n",
    "    term = diffrax.ODETerm(vector_field=_rhs_single)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        y0=h0_vec,\n",
    "        args=(ode_callable,),   # pass the jittable callable as single-element tuple\n",
    "        dt0=None,\n",
    "        max_steps=1_000_000,\n",
    "        adjoint=adjoint,\n",
    "        stepsize_controller=controller,\n",
    "        saveat=diffrax.SaveAt(t1=True),\n",
    "    )\n",
    "    y_final = sol.ys[0] if hasattr(sol.ys, \"__getitem__\") else sol.ys\n",
    "    return jnp.asarray(y_final).reshape((1, -1)).squeeze(axis=0)\n",
    "\n",
    "\n",
    "def integrate_batch(ode_module, h0, t0, t1):\n",
    "    \"\"\"\n",
    "    Integrate batch of hidden states by vmapping single-sample integrator.\n",
    "    ode_module: Equinox ODEFunc module\n",
    "    h0: (B, h_dim)\n",
    "    returns: (B, h_dim)\n",
    "    \"\"\"\n",
    "    h0 = jnp.atleast_2d(h0)\n",
    "    # vmap over first axis (batch)\n",
    "    integrate_vmapped = jax.vmap(lambda h: _integrate_single(ode_module, h, t0, t1))\n",
    "    return integrate_vmapped(h0)\n",
    "\n",
    "\n",
    "def mse(a, b):\n",
    "    return jnp.mean((a - b) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FIXED: sequence_loss using lax.fori_loop\n",
    "# -----------------------------\n",
    "def sequence_loss(model: ODERNN, x_seq, alpha_true, t_grid_static):\n",
    "    \"\"\"\n",
    "    Compute loss for a batch of sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: ODERNN model\n",
    "        x_seq: Input sequences (B, T, D)\n",
    "        alpha_true: True alpha values (B, 1)\n",
    "        t_grid_static: Time grid (T,) - passed as static argument\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss\n",
    "        alpha_pred: Predicted alpha values (B, 1)\n",
    "    \"\"\"\n",
    "    B, T_seq, D = x_seq.shape\n",
    "    \n",
    "    # Encode first observation\n",
    "    h_init = jax.vmap(model.encoder)(x_seq[:, 0, :])  # (B, h_dim)\n",
    "    \n",
    "    # Process sequence using lax.fori_loop for JIT compatibility\n",
    "    def step_fn(i, h):\n",
    "        # Integrate hidden state through time\n",
    "        t0 = t_grid_static[i]\n",
    "        t1 = t_grid_static[i + 1]\n",
    "        h_ode = integrate_batch(model.odefunc, h, t0, t1)\n",
    "        # Update with next observation\n",
    "        h_new = model.rnn_update(x_seq[:, i + 1, :], h_ode)\n",
    "        return h_new\n",
    "    \n",
    "    # Run loop from 0 to T-1\n",
    "    h_final = jax.lax.fori_loop(0, T_seq - 1, step_fn, h_init)\n",
    "    \n",
    "    # Decode to predict alpha\n",
    "    alpha_pred = model.decoder(h_final)\n",
    "    alpha_pred = alpha_pred.reshape((B, -1))\n",
    "\n",
    "    # Compute loss\n",
    "    loss = mse(alpha_pred, alpha_true)\n",
    "    \n",
    "    return loss, alpha_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training utilities\n",
    "# -----------------------------\n",
    "def loss_fn(model, x_batch, alpha_batch, t_grid_static):\n",
    "    \"\"\"Compute loss with L2 regularization.\"\"\"\n",
    "    loss, _ = sequence_loss(model, x_batch, alpha_batch, t_grid_static)\n",
    "    \n",
    "    # Add L2 regularization\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    leaves = jax.tree_util.tree_leaves(params)\n",
    "    l2 = 1e-6 * sum(jnp.sum(p ** 2) for p in leaves)\n",
    "    \n",
    "    return loss + l2\n",
    "\n",
    "\n",
    "# Create value_and_grad function\n",
    "@eqx.filter_jit\n",
    "def compute_loss_and_grad(model, x_batch, alpha_batch, t_grid_static):\n",
    "    \"\"\"Compute loss and gradients.\"\"\"\n",
    "    loss_val, grads = eqx.filter_value_and_grad(loss_fn)(model, x_batch, alpha_batch, t_grid_static)\n",
    "    return loss_val, grads\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def apply_updates(model, grads, opt_state, optimizer):\n",
    "    \"\"\"Apply gradient updates.\"\"\"\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state\n",
    "\n",
    "\n",
    "def data_loader(xy, alpha, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\"\"\"\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        b = idx[i:i+batch_size]\n",
    "        yield xy[b], alpha[b]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize model and optimizer\n",
    "# -----------------------------\n",
    "key = random.PRNGKey(0)\n",
    "h_dim = 64\n",
    "model = ODERNN(x_dim=D, h_dim=h_dim, key=key)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    \n",
    "    for xb, ab in data_loader(np.array(xy_train_n), np.array(alpha_train_n), batch_size):\n",
    "        xb_j = jnp.array(xb)\n",
    "        ab_j = jnp.array(ab)\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        loss_val, grads = compute_loss_and_grad(model, xb_j, ab_j, t_grid)\n",
    "        \n",
    "        # Apply updates\n",
    "        model, opt_state = apply_updates(model, grads, opt_state, optimizer)\n",
    "        \n",
    "        losses.append(float(loss_val))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} loss={np.mean(losses):.6f} time={t1-t0:.1f}s\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction function\n",
    "# -----------------------------\n",
    "def predict_alpha(model, xy_input, t_grid_static):\n",
    "    \"\"\"\n",
    "    Predict alpha values for input sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ODERNN model\n",
    "        xy_input: Input sequences (N, T, D) - normalized\n",
    "        t_grid_static: Time grid (T,)\n",
    "    \n",
    "    Returns:\n",
    "        alpha_pred: Predicted alpha values (N, 1) - denormalized\n",
    "    \"\"\"\n",
    "    batch_size_pred = 128\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, xy_input.shape[0], batch_size_pred):\n",
    "        xb = xy_input[i:i+batch_size_pred]\n",
    "        B = xb.shape[0]\n",
    "        T_seq = xb.shape[1]\n",
    "        \n",
    "        # Encode first observation\n",
    "        h = jax.vmap(model.encoder)(xb[:, 0, :])\n",
    "        \n",
    "        # Process sequence - use regular loop for prediction (not JIT-compiled)\n",
    "        for j in range(T_seq - 1):\n",
    "            t0 = float(t_grid_static[j])\n",
    "            t1 = float(t_grid_static[j + 1])\n",
    "            h = integrate_batch(model.odefunc, h, t0, t1)\n",
    "            h = model.rnn_update(xb[:, j + 1, :], h)\n",
    "        \n",
    "        # Decode to predict alpha\n",
    "        alpha_p = model.decoder(h)  # (B, 1) normalized\n",
    "        preds.append(np.array(alpha_p))\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    \n",
    "    # Denormalize\n",
    "    preds_orig = preds * alpha_std + alpha_mean\n",
    "    \n",
    "    return preds_orig\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate predictions and save\n",
    "# -----------------------------\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "alpha_test_pred = predict_alpha(model, xy_test_n, t_grid)\n",
    "np.save(\"alpha_test_pred.npy\", alpha_test_pred)\n",
    "print(f\"Saved alpha_test_pred.npy with shape {alpha_test_pred.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on training subset\n",
    "# -----------------------------\n",
    "print(\"\\nEvaluating on training subset...\")\n",
    "subset = 200\n",
    "train_pred = predict_alpha(model, xy_train_n[:subset], t_grid)\n",
    "train_true = np.array(alpha_train[:subset])\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(train_true.ravel(), train_pred.ravel(), alpha=0.6, s=20, label=\"Predicted vs True\")\n",
    "minv = min(train_true.min(), train_pred.min())\n",
    "maxv = max(train_true.max(), train_pred.max())\n",
    "plt.plot([minv, maxv], [minv, maxv], 'r--', linewidth=2, label=\"Identity line\")\n",
    "plt.xlabel(\"Alpha (True)\", fontsize=12)\n",
    "plt.ylabel(\"Alpha (Predicted)\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.title(\"Predicted Alpha vs True Alpha (Training Subset)\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print metrics\n",
    "mae = np.mean(np.abs(train_true - train_pred))\n",
    "rmse = np.sqrt(np.mean((train_true - train_pred) ** 2))\n",
    "corr = np.corrcoef(train_true.ravel(), train_pred.ravel())[0, 1]\n",
    "print(f\"\\nTraining subset metrics:\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"Correlation: {corr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7da9d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Terms are not compatible with solver! Got:\nODETerm(vector_field=<function _rhs_single>)\nbut expected:\ndiffrax.AbstractTerm\nNote that terms are checked recursively: if you scroll up you may find a root-cause error that is more specific.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:170\u001b[39m, in \u001b[36m_assert_term_compatible.<locals>._check\u001b[39m\u001b[34m(term_cls, term, term_contr_kwargs, yi)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     vf_type = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_eval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_eval_shape.py:38\u001b[39m, in \u001b[36mfilter_eval_shape\u001b[39m\u001b[34m(fun, *args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m dynamic, static = partition((fun, args, kwargs), _filter)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m dynamic_out, static_out = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combine(dynamic_out, static_out.value)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api.py:3062\u001b[39m, in \u001b[36meval_shape\u001b[39m\u001b[34m(fun, *args, **kwargs)\u001b[39m\n\u001b[32m   3061\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m: fun = partial(fun)\n\u001b[32m-> \u001b[39m\u001b[32m3062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.out_info\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:305\u001b[39m, in \u001b[36mjit_trace\u001b[39m\u001b[34m(jit_func, *args, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mjit_trace\u001b[39m(jit_func, *args, **kwargs) -> stages.Traced:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjit_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m   lower_callable = partial(_resolve_and_lower, args_flat, pgle_profiler=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_eval_shape.py:33\u001b[39m, in \u001b[36mfilter_eval_shape.<locals>._fn\u001b[39m\u001b[34m(_static, _dynamic)\u001b[39m\n\u001b[32m     32\u001b[39m _fun, _args, _kwargs = combine(_static, _dynamic)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m _out = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m _dynamic_out, _static_out = partition(_out, _filter)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_term.py:194\u001b[39m, in \u001b[36mODETerm.vf\u001b[39m\u001b[34m(self, t, y, args)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mvf\u001b[39m(\u001b[38;5;28mself\u001b[39m, t: RealScalarLike, y: Y, args: Args) -> _VF:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m jtu.tree_structure(out) != jtu.tree_structure(y):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36m_rhs_single\u001b[39m\u001b[34m(t, y, args)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# call the jitted Equinox-backed callable (it returns a (1, h_dim) array)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m dh = \u001b[43mode_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# return as 1D for diffrax when y0 is 1D\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:263\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    267\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    268\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    269\u001b[39m     const_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:136\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_python_pjit_helper\u001b[39m(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_flat:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:318\u001b[39m, in \u001b[36m_argnames_partial\u001b[39m\u001b[34m(_fun, _fixed_kwargs, *args, **dyn_kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m({k: v.val \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _fixed_kwargs.val.items()}, **dyn_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:292\u001b[39m, in \u001b[36m_argnums_partial\u001b[39m\u001b[34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(fixed_args_, sentinel) \u001b[38;5;129;01mis\u001b[39;00m sentinel\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_jit.py:55\u001b[39m, in \u001b[36m_filter_jit_cache.<locals>.fun_wrapped\u001b[39m\u001b[34m(dynamic_donate, dynamic_nodonate, static)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dummy_arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m out = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m dynamic_out, static_out = partition(out, is_array)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mmake_odefunc_callable.<locals>._callable\u001b[39m\u001b[34m(t, h, args_unused)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;129m@eqx\u001b[39m.filter_jit\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_callable\u001b[39m(t, h, args_unused):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mode_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mODEFunc.__call__\u001b[39m\u001b[34m(self, t, h, args)\u001b[39m\n\u001b[32m     78\u001b[39m inp = jnp.concatenate([h_mat, t_feat], axis=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# (B, h_dim)\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# return with same rank as input\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_mlp.py:140\u001b[39m, in \u001b[36mMLP.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers[:-\u001b[32m1\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     layer_activation = jtu.tree_map(\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[i] \u001b[38;5;28;01mif\u001b[39;00m is_array(x) \u001b[38;5;28;01melse\u001b[39;00m x, \u001b[38;5;28mself\u001b[39m.activation\n\u001b[32m    143\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\nn\\_linear.py:97\u001b[39m, in \u001b[36mLinear.__call__\u001b[39m\u001b[34m(self, x, key)\u001b[39m\n\u001b[32m     96\u001b[39m     x = jnp.broadcast_to(x, (\u001b[32m1\u001b[39m,))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:1141\u001b[39m, in \u001b[36m_forward_operator_to_aval.<locals>.op\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:604\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:263\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    267\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    268\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    269\u001b[39m     const_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:136\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_python_pjit_helper\u001b[39m(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m   p, args_flat = \u001b[43m_infer_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_flat:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:630\u001b[39m, in \u001b[36m_infer_params\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_params_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:655\u001b[39m, in \u001b[36m_infer_params_internal\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m dbg = dbg_fn()\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m p, args_flat = \u001b[43m_infer_params_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.params[\u001b[33m'\u001b[39m\u001b[33mjaxpr\u001b[39m\u001b[33m'\u001b[39m].jaxpr.is_high:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:551\u001b[39m, in \u001b[36m_infer_params_impl\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    549\u001b[39m qdd_token = _qdd_cache_index(flat_fun, in_type)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m jaxpr, consts, out_avals = \u001b[43m_create_pjit_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdd_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIgnoreKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mji\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:496\u001b[39m, in \u001b[36mcache.<locals>.memoized_fun\u001b[39m\u001b[34m(fun, *args)\u001b[39m\n\u001b[32m    495\u001b[39m   start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ans = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_explain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\pjit.py:1182\u001b[39m, in \u001b[36m_create_pjit_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m     jaxpr, global_out_avals, consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.debug_key_reuse.value:\n\u001b[32m   1185\u001b[39m   \u001b[38;5;66;03m# Import here to avoid circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2483\u001b[39m, in \u001b[36mtrace_to_jaxpr_dynamic\u001b[39m\u001b[34m(fun, in_avals, keep_inputs, lower, auto_dce)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m   ans = \u001b[43mfun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2484\u001b[39m _check_returned_jaxtypes(fun.debug_info, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:73\u001b[39m, in \u001b[36mflatten_fun\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     72\u001b[39m py_args, py_kwargs = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m ans, out_tree = tree_flatten(ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:318\u001b[39m, in \u001b[36m_argnames_partial\u001b[39m\u001b[34m(_fun, _fixed_kwargs, *args, **dyn_kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m({k: v.val \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _fixed_kwargs.val.items()}, **dyn_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\numpy\\tensor_contractions.py:254\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m    253\u001b[39m b = lax.squeeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m out = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m  \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m result = lax.transpose(out, perm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:2448\u001b[39m, in \u001b[36mdot_general\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   2442\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.lax.dot`.\u001b[39;00m\n\u001b[32m   2443\u001b[39m \n\u001b[32m   2444\u001b[39m \u001b[33;03mPrefer use of :func:`jax.lax.dot` directly, but note that it requires\u001b[39;00m\n\u001b[32m   2445\u001b[39m \u001b[33;03mall arguments after ``lhs`` and ``rhs`` to be specified by keyword\u001b[39;00m\n\u001b[32m   2446\u001b[39m \u001b[33;03mrather than position.\u001b[39;00m\n\u001b[32m   2447\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2448\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2449\u001b[39m \u001b[43m           \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:2557\u001b[39m, in \u001b[36mdot\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2556\u001b[39m lhs, rhs = core.standard_insert_pvary(lhs, rhs)\n\u001b[32m-> \u001b[39m\u001b[32m2557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot_general_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2559\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcanonicalize_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2561\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:632\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    631\u001b[39m args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:648\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:660\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    659\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2175\u001b[39m, in \u001b[36mDynamicJaxprTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, tracers, params)\u001b[39m\n\u001b[32m   2173\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m custom_staging_rules[primitive](\u001b[38;5;28mself\u001b[39m, source_info, *jaxpr_tracers,\n\u001b[32m   2174\u001b[39m                                          **params)\n\u001b[32m-> \u001b[39m\u001b[32m2175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_process_primitive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr_tracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2189\u001b[39m, in \u001b[36mDynamicJaxprTrace.default_process_primitive\u001b[39m\u001b[34m(self, primitive, tracers, params, source_info)\u001b[39m\n\u001b[32m   2188\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2189\u001b[39m   out_avals, effs = \u001b[43m_cached_abstract_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43maval_qdds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2191\u001b[39m   \u001b[38;5;66;03m# TODO(phawkins): remove this 3 months after the release of JAX v0.7.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\util.py:460\u001b[39m, in \u001b[36mmulti_weakref_lru_cache.<locals>.wrapper\u001b[39m\u001b[34m(*orig_args, **orig_kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nr_weakrefs == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_multi_weakref_placeholder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nr_weakrefs == \u001b[32m1\u001b[39m:\n\u001b[32m    463\u001b[39m   \u001b[38;5;66;03m# Put the single weakref first, and skip the MultiWeakRefCacheKey\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\util.py:444\u001b[39m, in \u001b[36mmulti_weakref_lru_cache.<locals>.cache_miss\u001b[39m\u001b[34m(key, *args, **kwargs)\u001b[39m\n\u001b[32m    442\u001b[39m   orig_args, orig_kwargs = sentinel_to_referrents(\n\u001b[32m    443\u001b[39m       (args, kwargs), \u001b[38;5;28miter\u001b[39m([weakref.ref(key)]), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43morig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\interpreters\\partial_eval.py:2004\u001b[39m, in \u001b[36m_cached_abstract_eval\u001b[39m\u001b[34m(primitive, *aval_qdds, **params)\u001b[39m\n\u001b[32m   2002\u001b[39m \u001b[38;5;129m@multi_weakref_lru_cache\u001b[39m\n\u001b[32m   2003\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_cached_abstract_eval\u001b[39m(primitive: core.Primitive, *aval_qdds, **params):\n\u001b[32m-> \u001b[39m\u001b[32m2004\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabstract_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43maval_qdds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\core.py:702\u001b[39m, in \u001b[36m_effect_free_abstract_eval.<locals>.abstract_eval_\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mabstract_eval_\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mabstract_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, no_effects\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\utils.py:168\u001b[39m, in \u001b[36mstandard_abstract_eval\u001b[39m\u001b[34m(prim, shape_rule, dtype_rule, weak_type_rule, sharding_rule, vma_rule, unreduced_rule, memory_space_rule, *avals, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m core.check_avals_context_mesh(avals, prim.name)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m out_shape, out_dtype, out_sharding = \u001b[43mcall_shape_dtype_sharding_rule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munreduced_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m out_vma = vma_rule(*avals, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\utils.py:111\u001b[39m, in \u001b[36mcall_shape_dtype_sharding_rule\u001b[39m\u001b[34m(prim, shape_rule, dtype_rule, sharding_rule, unreduced_rule, multi_out, *avals, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcall_shape_dtype_sharding_rule\u001b[39m(prim, shape_rule, dtype_rule, sharding_rule,\n\u001b[32m    110\u001b[39m                                    unreduced_rule, multi_out, *avals, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m   out_shapes = \u001b[43mshape_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mavals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m   out_dtypes = dtype_rule(*avals, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:5303\u001b[39m, in \u001b[36m_dot_general_shape_rule\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   5301\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5302\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5303\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[32m   5305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
      "\u001b[31mTypeError\u001b[39m: dot_general requires contracting dimensions to have the same shape, got (65,) and (1,).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:200\u001b[39m, in \u001b[36m_assert_term_compatible\u001b[39m\u001b[34m(t, y, args, terms, term_structure, contr_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m jax.numpy_dtype_promotion(\u001b[33m\"\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         \u001b[43mjtu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontr_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# ValueError may also arise from mismatched tree structures\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:364\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    363\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:364\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    363\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:172\u001b[39m, in \u001b[36m_assert_term_compatible.<locals>._check\u001b[39m\u001b[34m(term_cls, term, term_contr_kwargs, yi)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.vf: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m    173\u001b[39m vf_type_compatible = eqx.filter_eval_shape(\n\u001b[32m    174\u001b[39m     better_isinstance, vf_type, vf_type_expected\n\u001b[32m    175\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Error while tracing ODETerm(vector_field=<function _rhs_single>).vf: dot_general requires contracting dimensions to have the same shape, got (65,) and (1,).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 359\u001b[39m\n\u001b[32m    356\u001b[39m ab_j = jnp.array(ab)\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Compute loss and gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m loss_val, grads = \u001b[43mcompute_loss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# Apply updates\u001b[39;00m\n\u001b[32m    362\u001b[39m model, opt_state = apply_updates(model, grads, opt_state, optimizer)\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 308\u001b[39m, in \u001b[36mcompute_loss_and_grad\u001b[39m\u001b[34m(model, x_batch, alpha_batch, t_grid_static)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;129m@eqx\u001b[39m.filter_jit\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcompute_loss_and_grad\u001b[39m(model, x_batch, alpha_batch, t_grid_static):\n\u001b[32m    307\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss and gradients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     loss_val, grads = \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_grid_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_val, grads\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 294\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(model, x_batch, alpha_batch, t_grid_static)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mloss_fn\u001b[39m(model, x_batch, alpha_batch, t_grid_static):\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute loss with L2 regularization.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     loss, _ = \u001b[43msequence_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_grid_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# Add L2 regularization\u001b[39;00m\n\u001b[32m    297\u001b[39m     params = eqx.filter(model, eqx.is_array)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 277\u001b[39m, in \u001b[36msequence_loss\u001b[39m\u001b[34m(model, x_seq, alpha_true, t_grid_static)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h_new\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Run loop from 0 to T-1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m h_final = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfori_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# Decode to predict alpha\u001b[39;00m\n\u001b[32m    280\u001b[39m alpha_pred = model.decoder(h_final)\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 271\u001b[39m, in \u001b[36msequence_loss.<locals>.step_fn\u001b[39m\u001b[34m(i, h)\u001b[39m\n\u001b[32m    269\u001b[39m t0 = t_grid_static[i]\n\u001b[32m    270\u001b[39m t1 = t_grid_static[i + \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m h_ode = \u001b[43mintegrate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43modefunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Update with next observation\u001b[39;00m\n\u001b[32m    273\u001b[39m h_new = model.rnn_update(x_seq[:, i + \u001b[32m1\u001b[39m, :], h_ode)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mintegrate_batch\u001b[39m\u001b[34m(ode_module, h0, t0, t1)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# vmap over first axis (batch)\u001b[39;00m\n\u001b[32m    234\u001b[39m integrate_vmapped = jax.vmap(\u001b[38;5;28;01mlambda\u001b[39;00m h: _integrate_single(ode_module, h, t0, t1))\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mintegrate_vmapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 234\u001b[39m, in \u001b[36mintegrate_batch.<locals>.<lambda>\u001b[39m\u001b[34m(h)\u001b[39m\n\u001b[32m    232\u001b[39m h0 = jnp.atleast_2d(h0)\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# vmap over first axis (batch)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m integrate_vmapped = jax.vmap(\u001b[38;5;28;01mlambda\u001b[39;00m h: \u001b[43m_integrate_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m integrate_vmapped(h0)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 206\u001b[39m, in \u001b[36m_integrate_single\u001b[39m\u001b[34m(ode_module, h0_single, t0, t1)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Create a plain diffrax ODETerm here (diffrax expects an actual ODETerm instance)\u001b[39;00m\n\u001b[32m    204\u001b[39m term = diffrax.ODETerm(vector_field=_rhs_single)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m sol = \u001b[43mdiffrax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiffeqsolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh0_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# pass the jitted callable as argument to _rhs_single\u001b[39;49;00m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1_000_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstepsize_controller\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaveat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffrax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSaveAt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Extract final state (saved at t1)\u001b[39;00m\n\u001b[32m    221\u001b[39m y_final = sol.ys[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sol.ys, \u001b[33m\"\u001b[39m\u001b[33m__getitem__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m sol.ys\n",
      "    \u001b[31m[... skipping hidden 18 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:1103\u001b[39m, in \u001b[36mdiffeqsolve\u001b[39m\u001b[34m(terms, solver, t0, t1, dt0, y0, args, saveat, stepsize_controller, adjoint, event, max_steps, throw, progress_meter, solver_state, controller_state, made_jump, discrete_terminating_event)\u001b[39m\n\u001b[32m   1100\u001b[39m         terms = MultiTerm(*terms)\n\u001b[32m   1102\u001b[39m \u001b[38;5;66;03m# Error checking for term compatibility\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m \u001b[43m_assert_term_compatible\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterm_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterm_compatible_contr_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sde(terms):\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(solver, (AbstractItoSolver, AbstractStratonovichSolver)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffrax\\_integrate.py:205\u001b[39m, in \u001b[36m_assert_term_compatible\u001b[39m\u001b[34m(t, y, args, terms, term_structure, contr_kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m pretty_term = wl.pformat(terms)\n\u001b[32m    204\u001b[39m pretty_expected = wl.pformat(term_structure)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    206\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTerms are not compatible with solver! Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpretty_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbut expected:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpretty_expected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote that terms are checked recursively: if you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscroll up you may find a root-cause error that is more specific.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Terms are not compatible with solver! Got:\nODETerm(vector_field=<function _rhs_single>)\nbut expected:\ndiffrax.AbstractTerm\nNote that terms are checked recursively: if you scroll up you may find a root-cause error that is more specific."
     ]
    }
   ],
   "source": [
    "# ode_rnn_spiral_fixed.py\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train = data[\"xy_train\"].astype(np.float32)   # (N, T, 2)\n",
    "alpha_train = data[\"alpha_train\"].astype(np.float32)  # (N, 1)\n",
    "xy_test = data[\"xy_test\"].astype(np.float32)\n",
    "\n",
    "N, T, D = xy_train.shape\n",
    "assert D == 2\n",
    "\n",
    "# normalize inputs (per-dimension)\n",
    "xy_mean = xy_train.mean(axis=(0,1), keepdims=True)\n",
    "xy_std  = xy_train.std(axis=(0,1), keepdims=True) + 1e-6\n",
    "xy_train_n = (xy_train - xy_mean) / xy_std\n",
    "xy_test_n  = (xy_test - xy_mean) / xy_std\n",
    "\n",
    "alpha_mean = alpha_train.mean(axis=0, keepdims=True)\n",
    "alpha_std  = alpha_train.std(axis=0, keepdims=True) + 1e-6\n",
    "alpha_train_n = (alpha_train - alpha_mean) / alpha_std\n",
    "\n",
    "# convert to jax\n",
    "xy_train_n = jnp.array(xy_train_n)\n",
    "alpha_train_n = jnp.array(alpha_train_n)\n",
    "xy_test_n = jnp.array(xy_test_n)\n",
    "\n",
    "# temporal grid: assume observations evenly spaced in [0,1]\n",
    "t_grid = jnp.linspace(0.0, 1.0, T).astype(jnp.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Model components (Equinox)\n",
    "# -----------------------------\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=128, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        return self.net(x0)\n",
    "\n",
    "\n",
    "class ODEFunc(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, h_dim, *, key):\n",
    "        self.net = eqx.nn.MLP(\n",
    "            in_size=h_dim + 1,  # h_dim + 1 for time feature\n",
    "            out_size=h_dim,\n",
    "            width_size=128,\n",
    "            depth=2,\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, h, args):\n",
    "        # h: Input state (potentially 1D or 2D)\n",
    "        # We need to ensure that the output shape matches the input shape,\n",
    "        # but the internal logic requires a BATCH dimension.\n",
    "        \n",
    "        # Store original shape for output\n",
    "        original_shape = h.shape\n",
    "        h_mat = ensure_batched(h)      # (B, h_dim)\n",
    "        B = h_mat.shape[0]\n",
    "        t_feat = jnp.full((B, 1), jnp.asarray(t, dtype=h_mat.dtype))\n",
    "        inp = jnp.concatenate([h_mat, t_feat], axis=-1)\n",
    "        out = self.net(inp)            # (B, h_dim)\n",
    "        # return with same rank as input\n",
    "        if len(original_shape) == 1:\n",
    "            return out.reshape((-1,))\n",
    "        return out.reshape(original_shape)\n",
    "\n",
    "\n",
    "class Decoder(eqx.Module):\n",
    "    net: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, in_size, out_size, *, key):\n",
    "        self.net = eqx.nn.MLP(in_size, out_size, width_size=64, depth=2, key=key)\n",
    "\n",
    "    def __call__(self, h):\n",
    "        h_mat = ensure_batched(h)\n",
    "        out = self.net(h_mat)\n",
    "        # keep batch semantics: if input was 1D return (out_dim,) else (B, out_dim)\n",
    "        if h.ndim == 1:\n",
    "            return out.reshape((-1,))  # (out_dim,)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RNNUpdate(eqx.Module):\n",
    "    lin_x: eqx.nn.Linear\n",
    "    lin_h: eqx.nn.Linear\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, key):\n",
    "        k1, k2 = jax.random.split(key, 2)\n",
    "        self.lin_x = eqx.nn.Linear(x_dim, 3 * h_dim, key=k1)\n",
    "        self.lin_h = eqx.nn.Linear(h_dim, 3 * h_dim, key=k2)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        x = ensure_batched(x)\n",
    "        h = ensure_batched(h)\n",
    "        gates = self.lin_x(x) + self.lin_h(h)  # (B, 3*h_dim)\n",
    "        z, r, o = jnp.split(gates, 3, axis=-1)\n",
    "        z = jnn.sigmoid(z)\n",
    "        r = jnn.sigmoid(r)\n",
    "        o = jnp.tanh(o)\n",
    "        h_new = (1 - z) * h + z * o\n",
    "        return h_new\n",
    "\n",
    "\n",
    "class ODERNN(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    odefunc: ODEFunc\n",
    "    rnn_update: RNNUpdate\n",
    "    decoder: Decoder\n",
    "    h_dim: int\n",
    "\n",
    "    def __init__(self, x_dim, h_dim, *, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.encoder = Encoder(x_dim, h_dim, key=k1)\n",
    "        self.odefunc = ODEFunc(h_dim, key=k2)\n",
    "        self.rnn_update = RNNUpdate(x_dim, h_dim, key=k3)\n",
    "        self.decoder = Decoder(h_dim, 1, key=k4)\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "\n",
    "# Solver configuration\n",
    "solver = diffrax.Tsit5()\n",
    "adjoint = diffrax.BacksolveAdjoint()\n",
    "controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
    "# Erstelle eine jittbare wrapper-Funktion für das Equinox ODE-Modul.\n",
    "# Das macht das Modul in JAX-Tracing stabil verwendbar.\n",
    "def make_odefunc_callable(ode_module):\n",
    "    # eqx.filter_jit macht eine JIT-geeignete, tracbare Callable aus einem PyTree-gebundenen Funktions-Closure.\n",
    "    # Sie gibt eine Funktion f(t, h, args_unused) zurück, die direkt das Equinox-Modul aufruft.\n",
    "    @eqx.filter_jit\n",
    "    def _callable(t, h, args_unused):\n",
    "        return ode_module(t, h, None)\n",
    "    return _callable\n",
    "\n",
    "def ensure_batched(x):\n",
    "    x = jnp.asarray(x)\n",
    "    if x.ndim == 0:\n",
    "        return x.reshape((1, 1))\n",
    "    if x.ndim == 1:\n",
    "        return x[None, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Top-level RHS, erwartete Signatur für diffrax: rhs(t, y, args)\n",
    "def _rhs_flat_top_level(t, y, args):\n",
    "    \"\"\"\n",
    "    args is a tuple (ode_callable, B, h_dim)\n",
    "    y is flat vector length B*h_dim\n",
    "    \"\"\"\n",
    "    ode_callable, B_local, h_dim_local = args\n",
    "    y_mat = y.reshape((B_local, h_dim_local))\n",
    "    dydt = ode_callable(t, y_mat, None)\n",
    "    return dydt.ravel()\n",
    "\n",
    "# Top-level single-sample RHS: tracerfreundlich und einfach\n",
    "def _rhs_single(t, y, args):\n",
    "    # args: (ode_callable,)\n",
    "    ode_callable, = args\n",
    "    # ensure 1D -> 2D batch for the callable\n",
    "    h = jnp.asarray(y).reshape(-1)     # (h_dim,)\n",
    "    h_mat = h[None, :]                 # (1, h_dim)\n",
    "    # call the jitted Equinox-backed callable (it returns a (1, h_dim) array)\n",
    "    dh = ode_callable(t, h_mat, None)\n",
    "    # return as 1D for diffrax when y0 is 1D\n",
    "    return jnp.asarray(dh).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "def _integrate_single(ode_module, h0_single, t0, t1):\n",
    "    \"\"\"\n",
    "    Integrate one sample hidden state from t0 to t1.\n",
    "    ode_module: Equinox ODEFunc module\n",
    "    h0_single: (h_dim,) or (1, h_dim)\n",
    "    returns: h1_single shaped (h_dim,)\n",
    "    \"\"\"\n",
    "    # prepare initial vector as 1D\n",
    "    h0_vec = jnp.asarray(h0_single).reshape(-1)  # (h_dim,)\n",
    "\n",
    "    # Make a jittable callable from the Equinox module\n",
    "    ode_callable = make_odefunc_callable(ode_module)  # returns a JAX-jitted function\n",
    "\n",
    "    # Create a plain diffrax ODETerm here (diffrax expects an actual ODETerm instance)\n",
    "    term = diffrax.ODETerm(vector_field=_rhs_single)\n",
    "\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        y0=h0_vec,\n",
    "        args=(ode_callable,),   # pass the jitted callable as argument to _rhs_single\n",
    "        dt0=None,\n",
    "        max_steps=1_000_000,\n",
    "        adjoint=adjoint,\n",
    "        stepsize_controller=controller,\n",
    "        saveat=diffrax.SaveAt(t1=True),\n",
    "    )\n",
    "\n",
    "    # Extract final state (saved at t1)\n",
    "    y_final = sol.ys[0] if hasattr(sol.ys, \"__getitem__\") else sol.ys\n",
    "    return jnp.asarray(y_final).reshape((1, -1)).squeeze(axis=0)  # (h_dim,)\n",
    "\n",
    "\n",
    "def integrate_batch(ode_module, h0, t0, t1):\n",
    "    \"\"\"\n",
    "    Integrate batch of hidden states by vmapping single-sample integrator.\n",
    "    ode_module: Equinox ODEFunc module\n",
    "    h0: (B, h_dim)\n",
    "    returns: (B, h_dim)\n",
    "    \"\"\"\n",
    "    h0 = jnp.atleast_2d(h0)\n",
    "    # vmap over first axis (batch)\n",
    "    integrate_vmapped = jax.vmap(lambda h: _integrate_single(ode_module, h, t0, t1))\n",
    "    return integrate_vmapped(h0)\n",
    "\n",
    "\n",
    "def mse(a, b):\n",
    "    return jnp.mean((a - b) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FIXED: sequence_loss using lax.fori_loop\n",
    "# -----------------------------\n",
    "def sequence_loss(model: ODERNN, x_seq, alpha_true, t_grid_static):\n",
    "    \"\"\"\n",
    "    Compute loss for a batch of sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: ODERNN model\n",
    "        x_seq: Input sequences (B, T, D)\n",
    "        alpha_true: True alpha values (B, 1)\n",
    "        t_grid_static: Time grid (T,) - passed as static argument\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss\n",
    "        alpha_pred: Predicted alpha values (B, 1)\n",
    "    \"\"\"\n",
    "    B, T_seq, D = x_seq.shape\n",
    "    \n",
    "    # Encode first observation\n",
    "    h_init = jax.vmap(model.encoder)(x_seq[:, 0, :])  # (B, h_dim)\n",
    "    \n",
    "    # Process sequence using lax.fori_loop for JIT compatibility\n",
    "    def step_fn(i, h):\n",
    "        # Integrate hidden state through time\n",
    "        t0 = t_grid_static[i]\n",
    "        t1 = t_grid_static[i + 1]\n",
    "        h_ode = integrate_batch(model.odefunc, h, t0, t1)\n",
    "        # Update with next observation\n",
    "        h_new = model.rnn_update(x_seq[:, i + 1, :], h_ode)\n",
    "        return h_new\n",
    "    \n",
    "    # Run loop from 0 to T-1\n",
    "    h_final = jax.lax.fori_loop(0, T_seq - 1, step_fn, h_init)\n",
    "    \n",
    "    # Decode to predict alpha\n",
    "    alpha_pred = model.decoder(h_final)\n",
    "    alpha_pred = alpha_pred.reshape((B, -1))\n",
    "\n",
    "    # Compute loss\n",
    "    loss = mse(alpha_pred, alpha_true)\n",
    "    \n",
    "    return loss, alpha_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training utilities\n",
    "# -----------------------------\n",
    "def loss_fn(model, x_batch, alpha_batch, t_grid_static):\n",
    "    \"\"\"Compute loss with L2 regularization.\"\"\"\n",
    "    loss, _ = sequence_loss(model, x_batch, alpha_batch, t_grid_static)\n",
    "    \n",
    "    # Add L2 regularization\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    leaves = jax.tree_util.tree_leaves(params)\n",
    "    l2 = 1e-6 * sum(jnp.sum(p ** 2) for p in leaves)\n",
    "    \n",
    "    return loss + l2\n",
    "\n",
    "\n",
    "# Create value_and_grad function\n",
    "@eqx.filter_jit\n",
    "def compute_loss_and_grad(model, x_batch, alpha_batch, t_grid_static):\n",
    "    \"\"\"Compute loss and gradients.\"\"\"\n",
    "    loss_val, grads = eqx.filter_value_and_grad(loss_fn)(model, x_batch, alpha_batch, t_grid_static)\n",
    "    return loss_val, grads\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def apply_updates(model, grads, opt_state, optimizer):\n",
    "    \"\"\"Apply gradient updates.\"\"\"\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state\n",
    "\n",
    "\n",
    "def data_loader(xy, alpha, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\"\"\"\n",
    "    N = xy.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, N, batch_size):\n",
    "        b = idx[i:i+batch_size]\n",
    "        yield xy[b], alpha[b]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize model and optimizer\n",
    "# -----------------------------\n",
    "key = random.PRNGKey(0)\n",
    "h_dim = 64\n",
    "model = ODERNN(x_dim=D, h_dim=h_dim, key=key)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    \n",
    "    for xb, ab in data_loader(np.array(xy_train_n), np.array(alpha_train_n), batch_size):\n",
    "        xb_j = jnp.array(xb)\n",
    "        ab_j = jnp.array(ab)\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        loss_val, grads = compute_loss_and_grad(model, xb_j, ab_j, t_grid)\n",
    "        \n",
    "        # Apply updates\n",
    "        model, opt_state = apply_updates(model, grads, opt_state, optimizer)\n",
    "        \n",
    "        losses.append(float(loss_val))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} loss={np.mean(losses):.6f} time={t1-t0:.1f}s\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction function\n",
    "# -----------------------------\n",
    "def predict_alpha(model, xy_input, t_grid_static):\n",
    "    \"\"\"\n",
    "    Predict alpha values for input sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ODERNN model\n",
    "        xy_input: Input sequences (N, T, D) - normalized\n",
    "        t_grid_static: Time grid (T,)\n",
    "    \n",
    "    Returns:\n",
    "        alpha_pred: Predicted alpha values (N, 1) - denormalized\n",
    "    \"\"\"\n",
    "    batch_size_pred = 128\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, xy_input.shape[0], batch_size_pred):\n",
    "        xb = xy_input[i:i+batch_size_pred]\n",
    "        B = xb.shape[0]\n",
    "        T_seq = xb.shape[1]\n",
    "        \n",
    "        # Encode first observation\n",
    "        h = jax.vmap(model.encoder)(xb[:, 0, :])\n",
    "        \n",
    "        # Process sequence - use regular loop for prediction (not JIT-compiled)\n",
    "        for j in range(T_seq - 1):\n",
    "            t0 = float(t_grid_static[j])\n",
    "            t1 = float(t_grid_static[j + 1])\n",
    "            h = integrate_batch(model.odefunc, h, t0, t1)\n",
    "            h = model.rnn_update(xb[:, j + 1, :], h)\n",
    "        \n",
    "        # Decode to predict alpha\n",
    "        alpha_p = model.decoder(h)  # (B, 1) normalized\n",
    "        preds.append(np.array(alpha_p))\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    \n",
    "    # Denormalize\n",
    "    preds_orig = preds * alpha_std + alpha_mean\n",
    "    \n",
    "    return preds_orig\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate predictions and save\n",
    "# -----------------------------\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "alpha_test_pred = predict_alpha(model, xy_test_n, t_grid)\n",
    "np.save(\"alpha_test_pred.npy\", alpha_test_pred)\n",
    "print(f\"Saved alpha_test_pred.npy with shape {alpha_test_pred.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on training subset\n",
    "# -----------------------------\n",
    "print(\"\\nEvaluating on training subset...\")\n",
    "subset = 200\n",
    "train_pred = predict_alpha(model, xy_train_n[:subset], t_grid)\n",
    "train_true = np.array(alpha_train[:subset])\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(train_true.ravel(), train_pred.ravel(), alpha=0.6, s=20, label=\"Predicted vs True\")\n",
    "minv = min(train_true.min(), train_pred.min())\n",
    "maxv = max(train_true.max(), train_pred.max())\n",
    "plt.plot([minv, maxv], [minv, maxv], 'r--', linewidth=2, label=\"Identity line\")\n",
    "plt.xlabel(\"Alpha (True)\", fontsize=12)\n",
    "plt.ylabel(\"Alpha (Predicted)\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.title(\"Predicted Alpha vs True Alpha (Training Subset)\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print metrics\n",
    "mae = np.mean(np.abs(train_true - train_pred))\n",
    "rmse = np.sqrt(np.mean((train_true - train_pred) ** 2))\n",
    "corr = np.corrcoef(train_true.ravel(), train_pred.ravel())[0, 1]\n",
    "print(f\"\\nTraining subset metrics:\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"Correlation: {corr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf26a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_train shape: (10000, 100, 2)\n",
      "alpha_train shape: (10000, 1)\n",
      "xy_test shape: (10000, 100, 2)\n",
      "Starte Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[32m    220\u001b[39m train_key, loader_key = jr.split(train_key)\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m Xb, yb \u001b[38;5;129;01min\u001b[39;00m get_batches(xy_train, alpha_train, batch_size, loader_key):\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     model, opt_state, loss = \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     losses.append(loss)\n\u001b[32m    226\u001b[39m mean_loss = jnp.mean(jnp.array(losses))\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_module\\_module.py:110\u001b[39m, in \u001b[36m_ModuleFlattener.unflatten_with_cls\u001b[39m\u001b[34m(module_cls, aux, dynamic_field_values)\u001b[39m\n\u001b[32m    103\u001b[39m     aux = _FlattenedData(\n\u001b[32m    104\u001b[39m         \u001b[38;5;28mtuple\u001b[39m(dynamic_fs),\n\u001b[32m    105\u001b[39m         \u001b[38;5;28mtuple\u001b[39m([(k, get(k, _flatten_sentinel)) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.static_fs]),\n\u001b[32m    106\u001b[39m         \u001b[38;5;28mtuple\u001b[39m([(k, get(k, _flatten_sentinel)) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m wrapper_field_names]),\n\u001b[32m    107\u001b[39m     )\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(dynamic_vs), aux\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34munflatten_with_cls\u001b[39m(\n\u001b[32m    112\u001b[39m     module_cls: \u001b[38;5;28mtype\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    113\u001b[39m     aux: _FlattenedData,\n\u001b[32m    114\u001b[39m     dynamic_field_values: \u001b[38;5;28mtuple\u001b[39m[PyTree, ...],\n\u001b[32m    115\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# This doesn't go via `__init__`. A user may have done something\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# nontrivial there, and the field values may be dummy values as used in\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# various places throughout JAX. See also\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization,\u001b[39;00m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# which was (I believe) inspired by Equinox's approach here.\u001b[39;00m\n\u001b[32m    121\u001b[39m     module = \u001b[38;5;28mobject\u001b[39m.\u001b[34m__new__\u001b[39m(module_cls)\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(aux.dynamic_field_names, dynamic_field_values):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.tree_util as jtu\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Lade dataset spirals.npz\n",
    "# -----------------------------\n",
    "try:\n",
    "    data = np.load(\"spirals.npz\")\n",
    "    xy_train_np = data[\"xy_train\"]     # (10000, 100, 2)\n",
    "    alpha_train_np = data[\"alpha_train\"] # (10000, 1)\n",
    "    xy_test_np = data[\"xy_test\"]       # (10000, 100, 2)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: spirals.npz not found.\")\n",
    "    # Create dummy data to allow script to run\n",
    "    xy_train_np = np.random.rand(100, 100, 2)\n",
    "    alpha_train_np = np.random.rand(100, 1)\n",
    "    xy_test_np = np.random.rand(100, 100, 2)\n",
    "\n",
    "print(f\"xy_train shape: {xy_train_np.shape}\")\n",
    "print(f\"alpha_train shape: {alpha_train_np.shape}\")\n",
    "print(f\"xy_test shape: {xy_test_np.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Skalierung der (x, y) Daten\n",
    "# -----------------------------\n",
    "# Scaler needs 2D data (N_samples * N_timesteps, N_features)\n",
    "xy_train_flat = xy_train_np.reshape(-1, 2)\n",
    "\n",
    "scaler_xy = StandardScaler()\n",
    "xy_train_flat_s = scaler_xy.fit_transform(xy_train_flat)\n",
    "# Reshape back to (N_samples, N_timesteps, N_features)\n",
    "xy_train_s = xy_train_flat_s.reshape(xy_train_np.shape)\n",
    "\n",
    "# Transform test data\n",
    "xy_test_flat = xy_test_np.reshape(-1, 2)\n",
    "xy_test_flat_s = scaler_xy.transform(xy_test_flat)\n",
    "xy_test_s = xy_test_flat_s.reshape(xy_test_np.shape)\n",
    "\n",
    "# Konvertiere zu JAX-Arrays\n",
    "xy_train = jnp.array(xy_train_s, dtype=jnp.float32)\n",
    "alpha_train = jnp.array(alpha_train_np, dtype=jnp.float32)\n",
    "xy_test = jnp.array(xy_test_s, dtype=jnp.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# ODE-Funktion für latente Dynamik (dh/dt = f(h))\n",
    "# -----------------------------\n",
    "class ODEFunc(eqx.Module):\n",
    "    \"\"\"Defines the ODE dynamics for the hidden state: dh/dt = f(h)\"\"\"\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_dim, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_dim,\n",
    "            out_size=hidden_dim,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        # y is the hidden state h. t and args are unused but required by diffrax.\n",
    "        return self.mlp(y)\n",
    "\n",
    "# -----------------------------\n",
    "# ODE-RNN Encoder Modell\n",
    "# -----------------------------\n",
    "class ODERN_Encoder(eqx.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural ODE Encoder.\n",
    "    Processes a sequence (x_1, ..., x_N) and encodes it into a final hidden state h_N.\n",
    "    \"\"\"\n",
    "    hidden_dim: int\n",
    "    ode_func: ODEFunc\n",
    "    update_cell: eqx.nn.GRUCell\n",
    "    predictor: eqx.nn.Linear\n",
    "\n",
    "    # ODE solver settings\n",
    "    solver: diffrax.AbstractSolver\n",
    "    adjoint: diffrax.AbstractAdjoint\n",
    "    stepsize_controller: diffrax.AbstractStepSizeController\n",
    "\n",
    "    def __init__(self, data_dim, hidden_dim, ode_width, ode_depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        ode_key, gru_key, pred_key = jr.split(key, 3)\n",
    "\n",
    "        # 1. Die ODE-Dynamik für den latenten Zustand: dh/dt = f(h)\n",
    "        self.ode_func = ODEFunc(hidden_dim, ode_width, ode_depth, key=ode_key)\n",
    "\n",
    "        # 2. Die RNN-Update-Zelle: h_k_new = GRU(x_k, h_k_evolved)\n",
    "        self.update_cell = eqx.nn.GRUCell(data_dim, hidden_dim, key=gru_key)\n",
    "\n",
    "        # 3. Der finale Prädiktor: alpha = Linear(h_N)\n",
    "        self.predictor = eqx.nn.Linear(hidden_dim, 1, key=pred_key)\n",
    "\n",
    "        # Solver-Setup\n",
    "        self.solver = diffrax.Tsit5()\n",
    "        self.adjoint = diffrax.BacksolveAdjoint()\n",
    "        self.stepsize_controller = diffrax.PIDController(rtol=1e-3, atol=1e-6)\n",
    "\n",
    "    def _call_single(self, x_seq):\n",
    "        \"\"\"Verarbeitet eine einzelne Trajektorie (L, D)\"\"\"\n",
    "        \n",
    "        # Initialer verborgener Zustand\n",
    "        h0 = jnp.zeros((self.hidden_dim,))\n",
    "        \n",
    "        # Zeitintervall für JEDEN RNN-Schritt (dh. dt=1)\n",
    "        t0 = 0.0\n",
    "        t1 = 1.0\n",
    "        dt0 = 1.0\n",
    "        ode_term = diffrax.ODETerm(self.ode_func)\n",
    "\n",
    "        def scan_body(h_prev, x_k):\n",
    "            \"\"\"\n",
    "            Ein Schritt des ODE-RNN:\n",
    "            1. h_evolved = ODESolve(h_prev)\n",
    "            2. h_updated = GRUCell(x_k, h_evolved)\n",
    "            \"\"\"\n",
    "            # 1. Lasse den verborgenen Zustand sich entwickeln (ODE-Teil)\n",
    "            sol = diffrax.diffeqsolve(\n",
    "                ode_term,\n",
    "                self.solver,\n",
    "                t0=t0,\n",
    "                t1=t1,\n",
    "                dt0=dt0,\n",
    "                y0=h_prev,\n",
    "                stepsize_controller=self.stepsize_controller,\n",
    "                adjoint=self.adjoint,\n",
    "            )\n",
    "            h_evolved = sol.ys[-1]  # Nimm den Zustand am Ende des Intervalls\n",
    "\n",
    "            # 2. Aktualisiere den Zustand mit der Beobachtung (RNN-Teil)\n",
    "            h_updated = self.update_cell(x_k, h_evolved)\n",
    "            \n",
    "            return h_updated, h_updated # carry, output\n",
    "\n",
    "        # Iteriere über die Sequenzlänge (100 Schritte)\n",
    "        final_h, _ = jax.lax.scan(scan_body, init=h0, xs=x_seq)\n",
    "\n",
    "        # Mache die Vorhersage aus dem finalen verborgenen Zustand\n",
    "        pred_alpha = self.predictor(final_h)\n",
    "        return pred_alpha\n",
    "\n",
    "    def __call__(self, x_seq_batch):\n",
    "        \"\"\"Verarbeitet einen Batch von Trajektorien (B, L, D)\"\"\"\n",
    "        # Verwende vmap, um über die Batch-Dimension zu parallelisieren\n",
    "        return jax.vmap(self._call_single, in_axes=0)(x_seq_batch)\n",
    "\n",
    "# -----------------------------\n",
    "# Setup: Modell, Optimizer\n",
    "# -----------------------------\n",
    "key = jr.PRNGKey(0)\n",
    "data_dim = 2       # (x, y)\n",
    "hidden_dim = 5    # Dimension des latenten Zustands h\n",
    "ode_width = 64     # Breite der ODEFunc MLP\n",
    "ode_depth = 2      # Tiefe der ODEFunc MLP\n",
    "learning_rate = 1e-3\n",
    "l2_reg = 1e-5\n",
    "batch_size = 64\n",
    "epochs = 50        # Training kann lang dauern, starte mit 50\n",
    "\n",
    "model = ODERN_Encoder(\n",
    "    data_dim, hidden_dim, ode_width, ode_depth, key=jr.PRNGKey(123)\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(learning_rate, weight_decay=l2_reg)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "# -----------------------------\n",
    "# Loss, Optimizer, Update\n",
    "# -----------------------------\n",
    "def loss_fn(model, x, y):\n",
    "    \"\"\"MSE-Verlust für die Vorhersage von alpha.\"\"\"\n",
    "    preds = model(x) # (B, 1)\n",
    "    # L2-Regularisierung ist bereits im adamw optimizer als weight_decay\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "loss_and_grad = eqx.filter_value_and_grad(loss_fn)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(model, opt_state, x_batch, y_batch):\n",
    "    loss, grads = loss_and_grad(model, x_batch, y_batch)\n",
    "    params, static = eqx.partition(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "# -----------------------------\n",
    "# Data loader (JAX-basiert)\n",
    "# -----------------------------\n",
    "def get_batches(X, y, batch_size, key):\n",
    "    n = X.shape[0]\n",
    "    indices = jnp.arange(n)\n",
    "    indices = jr.permutation(key, indices)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "        b = indices[i : i + batch_size]\n",
    "        if len(b) == batch_size: # Nur volle Batches\n",
    "            yield X[b], y[b]\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "print(\"Starte Training...\")\n",
    "train_key = jr.PRNGKey(42)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = []\n",
    "    train_key, loader_key = jr.split(train_key)\n",
    "    \n",
    "    for Xb, yb in get_batches(xy_train, alpha_train, batch_size, loader_key):\n",
    "        model, opt_state, loss = update(model, opt_state, Xb, yb)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    mean_loss = jnp.mean(jnp.array(losses))\n",
    "    print(f\"Epoch {epoch:03d}/{epochs}   Loss={mean_loss:.6f}\")\n",
    "\n",
    "print(\"Training abgeschlossen.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Vorhersage und Speichern\n",
    "# -----------------------------\n",
    "print(\"Erstelle Vorhersagen für xy_test...\")\n",
    "# Wir müssen in Batches vorhersagen, um OOM-Fehler zu vermeiden\n",
    "test_batch_size = 256\n",
    "n_test = xy_test.shape[0]\n",
    "all_preds = []\n",
    "key = jr.PRNGKey(99) # Nicht benötigt, da get_batches nicht shuffelt\n",
    "\n",
    "def get_test_batches(X, batch_size):\n",
    "    n = X.shape[0]\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield X[i : i + batch_size]\n",
    "\n",
    "# Filter die 'update'-Funktion, um nur das Modell zu bekommen (für Inferenz)\n",
    "model_inf = eqx.filter(model, eqx.is_array)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, x_batch):\n",
    "    return model(x_batch)\n",
    "\n",
    "for Xb in get_test_batches(xy_test, test_batch_size):\n",
    "    preds_batch = predict_batch(model, Xb)\n",
    "    all_preds.append(np.array(preds_batch))\n",
    "\n",
    "# Kombiniere die Batch-Vorhersagen\n",
    "predicted_alphas_np = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Sicherstellen, dass die Form (10000, 1) ist\n",
    "if predicted_alphas_np.shape[0] != xy_test.shape[0]:\n",
    "    print(f\"Warnung: Anzahl Vorhersagen ({predicted_alphas_np.shape[0]}) stimmt nicht mit Test-Set ({xy_test.shape[0]}) überein.\")\n",
    "\n",
    "predicted_alphas_np = predicted_alphas_np.reshape(-1, 1)\n",
    "\n",
    "output_file = \"predicted_alphas.npy\"\n",
    "np.save(output_file, predicted_alphas_np)\n",
    "\n",
    "print(f\"Vorhersagen gespeichert in {output_file}\")\n",
    "print(f\"Form der Vorhersagen: {predicted_alphas_np.shape}\")\n",
    "\n",
    "# Zeige einige Vorhersagen\n",
    "print(\"\\nBeispiel-Vorhersagen:\")\n",
    "print(predicted_alphas_np[:10].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f585efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_train shape: (10000, 100, 2)\n",
      "alpha_train shape: (10000, 1)\n",
      "xy_test shape: (10000, 100, 2)\n",
      "Starte Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a callable value, got ODERN_Encoder(\n  hidden_dim=5,\n  ode_func=ODEFunc(\n    mlp=MLP(\n      layers=(\n        Linear(\n          weight=f32[64,5],\n          bias=f32[64],\n          in_features=5,\n          out_features=64,\n          use_bias=True\n        ),\n        Linear(\n          weight=f32[64,64],\n          bias=f32[64],\n          in_features=64,\n          out_features=64,\n          use_bias=True\n        ),\n        Linear(\n          weight=f32[5,64],\n          bias=f32[5],\n          in_features=64,\n          out_features=5,\n          use_bias=True\n        )\n      ),\n      activation=<PjitFunction of <function tanh at 0x0000028C7FE6B380>>,\n      final_activation=<function <lambda>>,\n      use_bias=True,\n      use_final_bias=True,\n      in_size=5,\n      out_size=5,\n      width_size=64,\n      depth=2\n    )\n  ),\n  update_cell=GRUCell(\n    weight_ih=f32[15,2],\n    weight_hh=f32[15,5],\n    bias=f32[15],\n    bias_n=f32[5],\n    input_size=2,\n    hidden_size=5,\n    use_bias=True\n  ),\n  predictor=Linear(\n    weight=f32[1,5], bias=f32[1], in_features=5, out_features=1, use_bias=True\n  ),\n  solver=Tsit5(),\n  adjoint=BacksolveAdjoint(kwargs={}),\n  stepsize_controller=PIDController(rtol=0.001, atol=1e-06)\n)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 218\u001b[39m\n\u001b[32m    215\u001b[39m train_key, loader_key = jr.split(train_key)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m Xb, yb \u001b[38;5;129;01min\u001b[39;00m get_batches(xy_train, alpha_train, batch_size, loader_key):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     model, opt_state, loss = \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     losses.append(loss)\n\u001b[32m    221\u001b[39m mean_loss = jnp.mean(jnp.array(losses))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mupdate\u001b[39m\u001b[34m(model, opt_state, x_batch, y_batch)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mupdate\u001b[39m(model, opt_state, x_batch, y_batch):\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     loss, grads = \u001b[43mloss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     params, static = eqx.partition(model, eqx.is_array)\n\u001b[32m    190\u001b[39m     updates, opt_state = optimizer.update(grads, opt_state, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\equinox\\_ad.py:71\u001b[39m, in \u001b[36m_ValueAndGradWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m x, *args = args\n\u001b[32m     70\u001b[39m diff_x, nondiff_x = partition(x, is_inexact_array)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnondiff_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 11 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(model, x, y)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mloss_fn\u001b[39m(model, x, y):\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     preds = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m(x) \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# L2-Regularisierung ist bereits im adamw optimizer als weight_decay\u001b[39;00m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.mean((preds - y) ** \u001b[32m2\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nilsl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api_util.py:460\u001b[39m, in \u001b[36mcheck_callable\u001b[39m\u001b[34m(fun)\u001b[39m\n\u001b[32m    458\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstaticmethod arguments are not supported, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fun):\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a callable value, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.isgeneratorfunction(fun):\n\u001b[32m    462\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a function, got a generator function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Expected a callable value, got ODERN_Encoder(\n  hidden_dim=5,\n  ode_func=ODEFunc(\n    mlp=MLP(\n      layers=(\n        Linear(\n          weight=f32[64,5],\n          bias=f32[64],\n          in_features=5,\n          out_features=64,\n          use_bias=True\n        ),\n        Linear(\n          weight=f32[64,64],\n          bias=f32[64],\n          in_features=64,\n          out_features=64,\n          use_bias=True\n        ),\n        Linear(\n          weight=f32[5,64],\n          bias=f32[5],\n          in_features=64,\n          out_features=5,\n          use_bias=True\n        )\n      ),\n      activation=<PjitFunction of <function tanh at 0x0000028C7FE6B380>>,\n      final_activation=<function <lambda>>,\n      use_bias=True,\n      use_final_bias=True,\n      in_size=5,\n      out_size=5,\n      width_size=64,\n      depth=2\n    )\n  ),\n  update_cell=GRUCell(\n    weight_ih=f32[15,2],\n    weight_hh=f32[15,5],\n    bias=f32[15],\n    bias_n=f32[5],\n    input_size=2,\n    hidden_size=5,\n    use_bias=True\n  ),\n  predictor=Linear(\n    weight=f32[1,5], bias=f32[1], in_features=5, out_features=1, use_bias=True\n  ),\n  solver=Tsit5(),\n  adjoint=BacksolveAdjoint(kwargs={}),\n  stepsize_controller=PIDController(rtol=0.001, atol=1e-06)\n)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.tree_util as jtu\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# Lade dataset spirals.npz\n",
    "# -----------------------------\n",
    "data = np.load(\"spirals.npz\")\n",
    "xy_train_np = data[\"xy_train\"]     # (10000, 100, 2)\n",
    "alpha_train_np = data[\"alpha_train\"] # (10000, 1)\n",
    "xy_test_np = data[\"xy_test\"]       # (10000, 100, 2)\n",
    "\n",
    "print(f\"xy_train shape: {xy_train_np.shape}\")\n",
    "print(f\"alpha_train shape: {alpha_train_np.shape}\")\n",
    "print(f\"xy_test shape: {xy_test_np.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Skalierung der (x, y) Daten\n",
    "# -----------------------------\n",
    "# Scaler needs 2D data (N_samples * N_timesteps, N_features)\n",
    "xy_train_flat = xy_train_np.reshape(-1, 2)\n",
    "\n",
    "scaler_xy = StandardScaler()\n",
    "xy_train_flat_s = scaler_xy.fit_transform(xy_train_flat)\n",
    "# Reshape back to (N_samples, N_timesteps, N_features)\n",
    "xy_train_s = xy_train_flat_s.reshape(xy_train_np.shape)\n",
    "\n",
    "# Transform test data\n",
    "xy_test_flat = xy_test_np.reshape(-1, 2)\n",
    "xy_test_flat_s = scaler_xy.transform(xy_test_flat)\n",
    "xy_test_s = xy_test_flat_s.reshape(xy_test_np.shape)\n",
    "\n",
    "# Konvertiere zu JAX-Arrays\n",
    "xy_train = jnp.array(xy_train_s, dtype=jnp.float32)\n",
    "alpha_train = jnp.array(alpha_train_np, dtype=jnp.float32)\n",
    "xy_test = jnp.array(xy_test_s, dtype=jnp.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# ODE-Funktion für latente Dynamik (dh/dt = f(h))\n",
    "# -----------------------------\n",
    "class ODEFunc(eqx.Module):\n",
    "    \"\"\"Defines the ODE dynamics for the hidden state: dh/dt = f(h)\"\"\"\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_dim, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_dim,\n",
    "            out_size=hidden_dim,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        # y is the hidden state h. t and args are unused but required by diffrax.\n",
    "        return self.mlp(y)\n",
    "\n",
    "# -----------------------------\n",
    "# ODE-RNN Encoder Modell\n",
    "# -----------------------------\n",
    "class ODERN_Encoder(eqx.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural ODE Encoder.\n",
    "    Processes a sequence (x_1, ..., x_N) and encodes it into a final hidden state h_N.\n",
    "    \"\"\"\n",
    "    hidden_dim: int\n",
    "    ode_func: ODEFunc\n",
    "    update_cell: eqx.nn.GRUCell\n",
    "    predictor: eqx.nn.Linear\n",
    "\n",
    "    # ODE solver settings\n",
    "    solver: diffrax.AbstractSolver\n",
    "    adjoint: diffrax.AbstractAdjoint\n",
    "    stepsize_controller: diffrax.AbstractStepSizeController\n",
    "\n",
    "    def __init__(self, data_dim, hidden_dim, ode_width, ode_depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        ode_key, gru_key, pred_key = jr.split(key, 3)\n",
    "\n",
    "        # 1. Die ODE-Dynamik für den latenten Zustand: dh/dt = f(h)\n",
    "        self.ode_func = ODEFunc(hidden_dim, ode_width, ode_depth, key=ode_key)\n",
    "\n",
    "        # 2. Die RNN-Update-Zelle: h_k_new = GRU(x_k, h_k_evolved)\n",
    "        self.update_cell = eqx.nn.GRUCell(data_dim, hidden_dim, key=gru_key)\n",
    "\n",
    "        # 3. Der finale Prädiktor: alpha = Linear(h_N)\n",
    "        self.predictor = eqx.nn.Linear(hidden_dim, 1, key=pred_key)\n",
    "\n",
    "        # Solver-Setup\n",
    "        self.solver = diffrax.Tsit5()\n",
    "        self.adjoint = diffrax.BacksolveAdjoint()\n",
    "        self.stepsize_controller = diffrax.PIDController(rtol=1e-3, atol=1e-6)\n",
    "\n",
    "    def _call_(self, x_seq):\n",
    "        \"\"\"Verarbeitet eine einzelne Trajektorie (L, D)\"\"\"\n",
    "        # Initialer verborgener Zustand\n",
    "        h0 = jnp.zeros((self.hidden_dim,))\n",
    "        \n",
    "        # Zeitintervall für JEDEN RNN-Schritt (dh. dt=1)\n",
    "        t0 = 0.0\n",
    "        t1 = 1.0\n",
    "        dt0 = 1.0\n",
    "        ode_term = diffrax.ODETerm(self.ode_func)\n",
    "\n",
    "        def scan_body(h_prev, x_k):\n",
    "            \"\"\"\n",
    "            Ein Schritt des ODE-RNN:\n",
    "            1. h_evolved = ODESolve(h_prev)\n",
    "            2. h_updated = GRUCell(x_k, h_evolved)\n",
    "            \"\"\"\n",
    "            # 1. Lasse den verborgenen Zustand sich entwickeln (ODE-Teil)\n",
    "            sol = diffrax.diffeqsolve(\n",
    "                ode_term,\n",
    "                self.solver,\n",
    "                t0=t0,\n",
    "                t1=t1,\n",
    "                dt0=dt0,\n",
    "                y0=h_prev,\n",
    "                stepsize_controller=self.stepsize_controller,\n",
    "                adjoint=self.adjoint,\n",
    "            )\n",
    "            h_evolved = sol.ys[-1]  # Nimm den Zustand am Ende des Intervalls\n",
    "\n",
    "            # 2. Aktualisiere den Zustand mit der Beobachtung (RNN-Teil)\n",
    "            h_updated = self.update_cell(x_k, h_evolved)\n",
    "            \n",
    "            return h_updated, h_updated # carry, output\n",
    "\n",
    "        # Iteriere über die Sequenzlänge (100 Schritte)\n",
    "        final_h, _ = jax.lax.scan(scan_body, init=h0, xs=x_seq)\n",
    "\n",
    "        # Mache die Vorhersage aus dem finalen verborgenen Zustand\n",
    "        pred_alpha = self.predictor(final_h)\n",
    "        return pred_alpha\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Setup: Modell, Optimizer\n",
    "# -----------------------------\n",
    "key = jr.PRNGKey(0)\n",
    "data_dim = 2       # (x, y)\n",
    "hidden_dim = 5    # Dimension des latenten Zustands h\n",
    "ode_width = 64     # Breite der ODEFunc MLP\n",
    "ode_depth = 2      # Tiefe der ODEFunc MLP\n",
    "learning_rate = 1e-3\n",
    "l2_reg = 1e-5\n",
    "batch_size = 64\n",
    "epochs = 50        # Training kann lang dauern, starte mit 50\n",
    "\n",
    "model = ODERN_Encoder(\n",
    "    data_dim, hidden_dim, ode_width, ode_depth, key=jr.PRNGKey(123)\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(learning_rate, weight_decay=l2_reg)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "# -----------------------------\n",
    "# Loss, Optimizer, Update\n",
    "# -----------------------------\n",
    "def loss_fn(model, x, y):\n",
    "    preds = jax.vmap(model, in_axes=0)(x) # (B, 1)\n",
    "    # L2-Regularisierung ist bereits im adamw optimizer als weight_decay\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "loss_and_grad = eqx.filter_value_and_grad(loss_fn)\n",
    "\n",
    "#@eqx.filter_jit\n",
    "def update(model, opt_state, x_batch, y_batch):\n",
    "    loss, grads = loss_and_grad(model, x_batch, y_batch)\n",
    "    params, static = eqx.partition(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "# -----------------------------\n",
    "# Data loader (JAX-basiert)\n",
    "# -----------------------------\n",
    "def get_batches(X, y, batch_size, key):\n",
    "    n = X.shape[0]\n",
    "    indices = jnp.arange(n)\n",
    "    indices = jr.permutation(key, indices)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "        b = indices[i : i + batch_size]\n",
    "        if len(b) == batch_size: # Nur volle Batches\n",
    "            yield X[b], y[b]\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "print(\"Starte Training...\")\n",
    "train_key = jr.PRNGKey(42)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = []\n",
    "    train_key, loader_key = jr.split(train_key)\n",
    "    \n",
    "    for Xb, yb in get_batches(xy_train, alpha_train, batch_size, loader_key):\n",
    "        model, opt_state, loss = update(model, opt_state, Xb, yb)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    mean_loss = jnp.mean(jnp.array(losses))\n",
    "    print(f\"Epoch {epoch:03d}/{epochs}   Loss={mean_loss:.6f}\")\n",
    "\n",
    "print(\"Training abgeschlossen.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Vorhersage und Speichern\n",
    "# -----------------------------\n",
    "print(\"Erstelle Vorhersagen für xy_test...\")\n",
    "# Wir müssen in Batches vorhersagen, um OOM-Fehler zu vermeiden\n",
    "test_batch_size = 256\n",
    "n_test = xy_test.shape[0]\n",
    "all_preds = []\n",
    "key = jr.PRNGKey(99) # Nicht benötigt, da get_batches nicht shuffelt\n",
    "\n",
    "def get_test_batches(X, batch_size):\n",
    "    n = X.shape[0]\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield X[i : i + batch_size]\n",
    "\n",
    "# Filter die 'update'-Funktion, um nur das Modell zu bekommen (für Inferenz)\n",
    "model_inf = eqx.filter(model, eqx.is_array)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, x_batch):\n",
    "    return model(x_batch)\n",
    "\n",
    "for Xb in get_test_batches(xy_test, test_batch_size):\n",
    "    preds_batch = predict_batch(model, Xb)\n",
    "    all_preds.append(np.array(preds_batch))\n",
    "\n",
    "# Kombiniere die Batch-Vorhersagen\n",
    "predicted_alphas_np = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Sicherstellen, dass die Form (10000, 1) ist\n",
    "if predicted_alphas_np.shape[0] != xy_test.shape[0]:\n",
    "    print(f\"Warnung: Anzahl Vorhersagen ({predicted_alphas_np.shape[0]}) stimmt nicht mit Test-Set ({xy_test.shape[0]}) überein.\")\n",
    "\n",
    "predicted_alphas_np = predicted_alphas_np.reshape(-1, 1)\n",
    "\n",
    "output_file = \"predicted_alphas.npy\"\n",
    "np.save(output_file, predicted_alphas_np)\n",
    "\n",
    "print(f\"Vorhersagen gespeichert in {output_file}\")\n",
    "print(f\"Form der Vorhersagen: {predicted_alphas_np.shape}\")\n",
    "\n",
    "# Zeige einige Vorhersagen\n",
    "print(\"\\nBeispiel-Vorhersagen:\")\n",
    "print(predicted_alphas_np[:10].flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
