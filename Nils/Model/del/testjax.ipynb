{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97bf434",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import jax  \n",
    "    import jax.numpy as jnp\n",
    "    import jaxlib\n",
    "    import diffrax\n",
    "    import equinox\n",
    "    import optax\n",
    "    import matplotlib\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "except:\n",
    "    import sys\n",
    "    import subprocess\n",
    "\n",
    "    packages = [\"jaxlib\", \"diffrax\", \"equinox\", \"optax\", \"matplotlib\", \"scikit-learn\"]\n",
    "\n",
    "    for pkg in packages:\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg]\n",
    "        print(\"Installiere:\", pkg)\n",
    "        subprocess.run(cmd)\n",
    "    print(\"Fertig.\")\n",
    "\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.tree_util as jtu\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Lade dataset spirals.npz\n",
    "# -----------------------------\n",
    "try:\n",
    "    data = np.load(\"spirals.npz\")\n",
    "    xy_train_np = data[\"xy_train\"]     # (10000, 100, 2)\n",
    "    alpha_train_np = data[\"alpha_train\"] # (10000, 1)\n",
    "    xy_test_np = data[\"xy_test\"]       # (10000, 100, 2)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: spirals.npz not found.\")\n",
    "    # Create dummy data to allow script to run\n",
    "    xy_train_np = np.random.rand(100, 100, 2)\n",
    "    alpha_train_np = np.random.rand(100, 1)\n",
    "    xy_test_np = np.random.rand(100, 100, 2)\n",
    "\n",
    "print(f\"xy_train shape: {xy_train_np.shape}\")\n",
    "print(f\"alpha_train shape: {alpha_train_np.shape}\")\n",
    "print(f\"xy_test shape: {xy_test_np.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Skalierung der (x, y) Daten\n",
    "# -----------------------------\n",
    "# Scaler needs 2D data (N_samples * N_timesteps, N_features)\n",
    "xy_train_flat = xy_train_np.reshape(-1, 2)\n",
    "\n",
    "scaler_xy = StandardScaler()\n",
    "xy_train_flat_s = scaler_xy.fit_transform(xy_train_flat)\n",
    "# Reshape back to (N_samples, N_timesteps, N_features)\n",
    "xy_train_s = xy_train_flat_s.reshape(xy_train_np.shape)\n",
    "\n",
    "# Transform test data\n",
    "xy_test_flat = xy_test_np.reshape(-1, 2)\n",
    "xy_test_flat_s = scaler_xy.transform(xy_test_flat)\n",
    "xy_test_s = xy_test_flat_s.reshape(xy_test_np.shape)\n",
    "\n",
    "# Konvertiere zu JAX-Arrays\n",
    "xy_train = jnp.array(xy_train_s, dtype=jnp.float32)\n",
    "alpha_train = jnp.array(alpha_train_np, dtype=jnp.float32)\n",
    "xy_test = jnp.array(xy_test_s, dtype=jnp.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# ODE-Funktion für latente Dynamik (dh/dt = f(h))\n",
    "# -----------------------------\n",
    "class ODEFunc(eqx.Module):\n",
    "    \"\"\"Defines the ODE dynamics for the hidden state: dh/dt = f(h)\"\"\"\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_dim, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_dim,\n",
    "            out_size=hidden_dim,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        # y is the hidden state h. t and args are unused but required by diffrax.\n",
    "        return self.mlp(y)\n",
    "\n",
    "# -----------------------------\n",
    "# ODE-RNN Encoder Modell\n",
    "# -----------------------------\n",
    "class ODERN_Encoder(eqx.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural ODE Encoder.\n",
    "    Processes a sequence (x_1, ..., x_N) and encodes it into a final hidden state h_N.\n",
    "    \"\"\"\n",
    "    hidden_dim: int\n",
    "    ode_func: ODEFunc\n",
    "    update_cell: eqx.nn.GRUCell\n",
    "    predictor: eqx.nn.Linear\n",
    "\n",
    "    # ODE solver settings\n",
    "    solver: diffrax.AbstractSolver\n",
    "    adjoint: diffrax.AbstractAdjoint\n",
    "    stepsize_controller: diffrax.AbstractStepSizeController\n",
    "\n",
    "    def __init__(self, data_dim, hidden_dim, ode_width, ode_depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        ode_key, gru_key, pred_key = jr.split(key, 3)\n",
    "\n",
    "        # 1. Die ODE-Dynamik für den latenten Zustand: dh/dt = f(h)\n",
    "        self.ode_func = ODEFunc(hidden_dim, ode_width, ode_depth, key=ode_key)\n",
    "\n",
    "        # 2. Die RNN-Update-Zelle: h_k_new = GRU(x_k, h_k_evolved)\n",
    "        self.update_cell = eqx.nn.GRUCell(data_dim, hidden_dim, key=gru_key)\n",
    "\n",
    "        # 3. Der finale Prädiktor: alpha = Linear(h_N)\n",
    "        self.predictor = eqx.nn.Linear(hidden_dim, 1, key=pred_key)\n",
    "\n",
    "        # Solver-Setup\n",
    "        self.solver = diffrax.Tsit5()\n",
    "        self.adjoint = diffrax.BacksolveAdjoint()\n",
    "        self.stepsize_controller = diffrax.PIDController(rtol=1e-2, atol=1e-4)\n",
    "\n",
    "    def __call__(self, x_seq):\n",
    "        \"\"\"Verarbeitet eine einzelne Trajektorie (L, D)\"\"\"\n",
    "        # Initialer verborgener Zustand\n",
    "        h0 = jnp.zeros((self.hidden_dim,))\n",
    "        \n",
    "        # Zeitintervall für JEDEN RNN-Schritt (dh. dt=1)\n",
    "        t0 = 0.0\n",
    "        t1 = 1.0\n",
    "        dt0 = 1.0\n",
    "        ode_term = diffrax.ODETerm(self.ode_func)\n",
    "\n",
    "        def scan_body(h_prev, x_k):\n",
    "            \"\"\"\n",
    "            Ein Schritt des ODE-RNN:\n",
    "            1. h_evolved = ODESolve(h_prev)\n",
    "            2. h_updated = GRUCell(x_k, h_evolved)\n",
    "            \"\"\"\n",
    "            # 1. Lasse den verborgenen Zustand sich entwickeln (ODE-Teil)\n",
    "            sol = diffrax.diffeqsolve(\n",
    "                ode_term,\n",
    "                self.solver,\n",
    "                t0=t0,\n",
    "                t1=t1,\n",
    "                dt0=dt0,\n",
    "                y0=h_prev,\n",
    "                stepsize_controller=self.stepsize_controller,\n",
    "                adjoint=self.adjoint,\n",
    "            )\n",
    "            h_evolved = sol.ys[-1]  # Nimm den Zustand am Ende des Intervalls\n",
    "\n",
    "            # 2. Aktualisiere den Zustand mit der Beobachtung (RNN-Teil)\n",
    "            h_updated = self.update_cell(x_k, h_evolved)\n",
    "            \n",
    "            return h_updated, h_updated # carry, output\n",
    "\n",
    "        # Iteriere über die Sequenzlänge (100 Schritte)\n",
    "        final_h, _ = jax.lax.scan(scan_body, init=h0, xs=x_seq)\n",
    "\n",
    "        # Mache die Vorhersage aus dem finalen verborgenen Zustand\n",
    "        pred_alpha = self.predictor(final_h)\n",
    "        return pred_alpha\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Setup: Modell, Optimizer\n",
    "# -----------------------------\n",
    "key = jr.PRNGKey(int(time.time()) + 65)\n",
    "data_dim = 2       # (x, y)\n",
    "hidden_dim = 16    # Dimension des latenten Zustands h\n",
    "ode_width = 32     # Breite der ODEFunc MLP\n",
    "ode_depth = 2      # Tiefe der ODEFunc MLP\n",
    "learning_rate = 1e-3\n",
    "l2_reg = 1e-5\n",
    "batch_size = 64\n",
    "epochs = 50        # Training kann lang dauern, starte mit 50\n",
    "\n",
    "model = ODERN_Encoder(\n",
    "    data_dim, hidden_dim, ode_width, ode_depth, key=jr.PRNGKey(int(time.time())+3)\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(learning_rate, weight_decay=l2_reg)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "# -----------------------------\n",
    "# Loss, Optimizer, Update\n",
    "# -----------------------------\n",
    "def loss_fn(model, x, y):\n",
    "    preds = jax.vmap(model, in_axes=0)(x) # (B, 1)\n",
    "    # L2-Regularisierung ist bereits im adamw optimizer als weight_decay\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "loss_and_grad = eqx.filter_value_and_grad(loss_fn)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(model, opt_state, x_batch, y_batch):\n",
    "    loss, grads = loss_and_grad(model, x_batch, y_batch)\n",
    "    params, static = eqx.partition(model, eqx.is_array)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "# -----------------------------\n",
    "# Data loader (JAX-basiert)\n",
    "# -----------------------------\n",
    "def get_batches(X, y, batch_size, key):\n",
    "    n = X.shape[0]\n",
    "    indices = jnp.arange(n)\n",
    "    indices = jr.permutation(key, indices)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "        b = indices[i : i + batch_size]\n",
    "        if len(b) == batch_size: # Nur volle Batches\n",
    "            yield X[b], y[b]\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "print(\"Starte Training...\")\n",
    "train_key = jr.PRNGKey(int(time.time()))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = []\n",
    "    train_key, loader_key = jr.split(train_key)\n",
    "    \n",
    "    for Xb, yb in get_batches(xy_train, alpha_train, batch_size, loader_key):\n",
    "        model, opt_state, loss = update(model, opt_state, Xb, yb)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    mean_loss = jnp.mean(jnp.array(losses))\n",
    "    print(f\"Epoch {epoch:03d}/{epochs}   Loss={mean_loss:.6f}\")\n",
    "\n",
    "print(\"Training abgeschlossen.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Vorhersage und Speichern\n",
    "# -----------------------------\n",
    "print(\"Erstelle Vorhersagen für xy_test...\")\n",
    "# Wir müssen in Batches vorhersagen, um OOM-Fehler zu vermeiden\n",
    "test_batch_size = 256\n",
    "n_test = xy_test.shape[0]\n",
    "all_preds = []\n",
    "key = jr.PRNGKey(99) # Nicht benötigt, da get_batches nicht shuffelt\n",
    "\n",
    "def get_test_batches(X, batch_size):\n",
    "    n = X.shape[0]\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield X[i : i + batch_size]\n",
    "\n",
    "# Filter die 'update'-Funktion, um nur das Modell zu bekommen (für Inferenz)\n",
    "model_inf = eqx.filter(model, eqx.is_array)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, x_batch):\n",
    "    return jax.vmap(model, in_axes = 0)(x_batch)\n",
    "\n",
    "for Xb in get_test_batches(xy_test, test_batch_size):\n",
    "    preds_batch = predict_batch(model, Xb)\n",
    "    all_preds.append(np.array(preds_batch))\n",
    "\n",
    "# Kombiniere die Batch-Vorhersagen\n",
    "predicted_alphas_np = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Sicherstellen, dass die Form (10000, 1) ist\n",
    "if predicted_alphas_np.shape[0] != xy_test.shape[0]:\n",
    "    print(f\"Warnung: Anzahl Vorhersagen ({predicted_alphas_np.shape[0]}) stimmt nicht mit Test-Set ({xy_test.shape[0]}) überein.\")\n",
    "\n",
    "predicted_alphas_np = predicted_alphas_np.reshape(-1, 1)\n",
    "\n",
    "output_file = \"predicted_alphas.npy\"\n",
    "np.save(output_file, predicted_alphas_np)\n",
    "\n",
    "print(f\"Vorhersagen gespeichert in {output_file}\")\n",
    "print(f\"Form der Vorhersagen: {predicted_alphas_np.shape}\")\n",
    "\n",
    "# Zeige einige Vorhersagen\n",
    "print(\"\\nBeispiel-Vorhersagen:\")\n",
    "print(predicted_alphas_np[:10].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec354bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "print(jax.devices())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
